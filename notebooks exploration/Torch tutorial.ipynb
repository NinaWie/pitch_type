{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch mit Lua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select(2,3)\n",
    "# of a tensor gives the 3 column\n",
    "select(1,3)\n",
    "#of a tensor yields 3 row\n",
    "\n",
    "#tensor represented in memory by rows\n",
    "a = torch.DoubleTensor(4,6) # uninitialized tensor containing doubles \n",
    "a = torch.CudaTensor(4,6)\n",
    "a = torch.Tensor(2,3,2)\n",
    "v = torch.Tensor{1,2,3,4} # für vector mit diesen einträgen\n",
    "# zweidimensional dann mit \n",
    "w = torch.Tensor{ {1,2,3}, {4,5,6}}\n",
    "v = torch.range(1,4) # oder torch.range(start, stop, step)\n",
    "v = torch.range(start, stop, anzahl)\n",
    "v = torch.logspace # logarithmic trend\n",
    "v = torch.zeros(3,3)\n",
    "v = torch.eye(3) # diagonalmatrix\n",
    "\n",
    "v:size(1) # size in first dimension\n",
    "v:dim() # gibt anzahl dimensionen, also 1\n",
    "v:double() # macht es zum DoubleTensor\n",
    "    \n",
    "v[3] und v[-2] # Zugriff auf tensors wie in numpy\n",
    "#Zugriff auf Elemente 2 bis 4:\n",
    "v[{ {2,4} }]\n",
    "w({ {}, {1} }) # alle rows und erste column elements, ergibt tensor mit size 2*1\n",
    "w({ {}, 1 }) # genauso aber ergibt tensor mit size 2, also extracted\n",
    "    \n",
    "torch.type(a) # gives type of a, default Double\n",
    "\n",
    "a:uniform() # fills a with uniform noise\n",
    "a:fill(0) #fills a with nullen\n",
    "# sequenz von 1 bis länge rein tun: Function definieren\n",
    "t:apply(function() i=i+1; return i; end)\n",
    "        \n",
    "b = a:select(1,3) # b is third row of a\n",
    "# wenn jetzt was an b bearbeitet wird, dann auch in a geändert! sozusagen pointer auf eine row/column\n",
    "# für Deep Copy\n",
    "a:resize(3,8)\n",
    "y = torch.sqrt(b)\n",
    "\n",
    "require \"cutorch\" # um auf cuda gpu\n",
    "\n",
    "model = nn.Sequential() # für linear\n",
    "\"\"\"\n",
    "model = nn.Concat() # für concatenation\n",
    "model = nn.Parallel() # für zwei paralle nwtzwerke\n",
    "\"\"\"\n",
    "model:add(nn.SpatialConvolution(3,16,5,5))\n",
    "model:add(nn.Tanh())\n",
    "model:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "model:add(nn.Linear(100, 1000)) # fully connected von 100 auf 1000 nodes\n",
    "\n",
    "    \n",
    "input= torch.CudaTensor(100)\n",
    "output = model:forward(input)\n",
    "    \n",
    "# Optim Package for AdamGrad usw\n",
    "\n",
    "# Scalarprodukt: einfach *\n",
    "v*w\n",
    "#VISUALISIERUNG\n",
    "require \"gnuplot\";\n",
    "\n",
    "gnuplot.plot(v);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
