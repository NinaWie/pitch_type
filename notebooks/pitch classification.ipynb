{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practise Pandas and clean merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_16.csv\")\n",
    "# print(file.columns.tolist())\n",
    "# print(file[\"Extension (P)\"][4])\n",
    "# file.dtypes()\n",
    "# print(file.head())\n",
    "# f체r array nur von den Daten: file.values()\n",
    "# file.sort_index(axis = 1, ascending = True) //um 체berschriften zu sortieren zb nach alphabet\n",
    "#sorted = file.sort_values(by = \"Extension (P)\")\n",
    "# print(a[\"Extension (P)\"])\n",
    "# f체r statistics also mean std usw: file.describe()\n",
    "# print(file[0:3])\n",
    "# mehrere spalten ausw채hlen: file.loc[:, ['A','B']]\n",
    "\n",
    "\n",
    "# a= df['Extension (P)'].values\n",
    "# print(a)\n",
    "# print(np.any(np.isnan(a)))\n",
    "\n",
    "df = df.sort_values(by = \"Backspin Rate (P)\")\n",
    "print(df[\"Backspin Rate (P)\"])\n",
    "for i in range(2289,2269, -1):\n",
    "    df = df.drop(df.index[[i]])\n",
    "print(df[\"Backspin Rate (P)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "koord = df.iloc[:,df.columns.get_loc(\"0\"):df.columns.get_loc(\"159\")]\n",
    "\n",
    "\"\"\"clean data by deleting nan columns\"\"\"\n",
    "for col in df.columns.tolist():\n",
    "    if (type(df[col][0]) is not np.float64 and type(df[col][0]) is not np.int64) or np.any(np.isnan(df[col].values)):\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "for col in koord.columns.tolist():\n",
    "    df[col]=koord[col]\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_clean.csv\")\n",
    "#print(cl.columns.tolist())\n",
    "#print(cl[\"0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_data.csv\")\n",
    "sv = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/sv_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get coordinate values and pitch type labels seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coordinates = cf.iloc[:,cf.columns.get_loc(\"0\"):cf.columns.get_loc(\"166\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique  = np.unique(cf[\"Pitch Type\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneHot(dataframe, column):\n",
    "    unique  = np.unique(dataframe[column].values)\n",
    "    l = len(dataframe.index)\n",
    "    loc = dataframe.columns.get_loc(column)\n",
    "    labels = np.zeros((l, 12))\n",
    "    for i in range(l):\n",
    "        #print(cf.iloc[i,loc])\n",
    "        pitch = dataframe.iloc[i,loc]\n",
    "        ind = unique.tolist().index(pitch)\n",
    "        labels[i, ind] = 1\n",
    "    return labels, unique\n",
    "\n",
    "labels, _ = oneHot(cf, \"Pitch Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_array = coordinates.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change pandas dataframe to np.array\n",
    "(Problem: missing values in den Koordinaten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M, N = data_array.shape\n",
    "data = np.zeros((M,N,18,2))\n",
    "\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        if not pd.isnull(data_array[i,j]):\n",
    "            data[i,j]=np.array(eval(data_array[i,j]))\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing = pd.isnull(data_array)\n",
    "print(\"Ratio missing: \", np.count_nonzero(missing)/(M*N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = data[:13000,:]\n",
    "test_x = data[13000:, :]\n",
    "train_t= labels[:13000,:]\n",
    "test_t = labels[13000:, :]\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, N, 18, 2), name = \"input\")\n",
    "x_ = tf.reshape(x, (-1, N, 36 ,1))\n",
    "y = tf.placeholder(tf.float32, (None, len(labels[0])))\n",
    "\n",
    "net = tf.layers.conv2d(x_, filters=16, kernel_size=5, strides=2, activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=16, kernel_size=3, strides=1, activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=32, kernel_size=3, strides=1, activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=1, kernel_size=1)\n",
    "shapes = net.get_shape().as_list()\n",
    "ff = tf.reshape(net, (-1, shapes[1]*shapes[2]))\n",
    "ff = tf.layers.dense(ff, 128, activation = tf.nn.relu)\n",
    "ff = tf.layers.dense(ff, len(labels[0]), activation = tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y-ff))\n",
    "optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batches(x, y, batchsize=32):\n",
    "    permute = np.random.permutation(len(x))\n",
    "    for i in range(0, len(x)-batchsize, batchsize):\n",
    "        indices = permute[i:i+batchsize]\n",
    "        yield x[indices], y[indices]\n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run session for 2000 epochs\n",
    "for epoch in range(2000 + 1):\n",
    "    for batch_x, batch_t in batches(train_x, train_t, 32):\n",
    "        sess.run(optimizer, {x: batch_x, y: batch_t})\n",
    "    print(epoch, sess.run(loss, {x: test_x, y: test_t}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sess.run(loss, {x: test_x, y: test_t}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode_one_hot(results):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    for pitch, i in enumerate(results):\n",
    "        ind = np.argmax(pitch)\n",
    "        p.append(unique[ind])\n",
    "    return p\n",
    "\n",
    "testing = sess.run(ff, {x: test_x, y: test_t})\n",
    "pitches = decode_one_hot(testing)\n",
    "label_pitches = (cf[\"Pitch Type\"].values)[13000:]\n",
    "\n",
    "# evaluate error rate\n",
    "right = 0\n",
    "wrong = 0\n",
    "for i in range(len(pitches)):\n",
    "    if pitches[i]==label_pitches[i]:\n",
    "        right+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "print(right/(right+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(dataframe, column):\n",
    "    unique  = np.unique(dataframe[column].values)\n",
    "    l = len(dataframe.index)\n",
    "    loc = dataframe.columns.get_loc(column)\n",
    "    labels = np.zeros((l, 12))\n",
    "    for i in range(l):\n",
    "        #print(cf.iloc[i,loc])\n",
    "        pitch = dataframe.iloc[i,loc]\n",
    "        ind = unique.tolist().index(pitch)\n",
    "        labels[i, ind] = 1\n",
    "    return labels, unique\n",
    "\n",
    "def decode_one_hot(results, unique):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    #unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    print(results)\n",
    "    for i, pitch in enumerate(results):\n",
    "        #print(pitch)\n",
    "        ind = np.argmax(pitch)\n",
    "        #print(ind)\n",
    "        p.append(unique[ind])\n",
    "    return p\n",
    "\n",
    "labels, unique = oneHot(frame, \"Pitch Type\")\n",
    "\n",
    "new = decode_one_hot(labels, unique)\n",
    "print(np.sum(new==frame[\"Pitch Type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only pitcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_data.csv\")\n",
    "#df = df.sort_values(by = \"Player\") # Ball Picther or Strike outcome pitcher\n",
    "# bis 8615 ball/pitcher\n",
    "pitcher = df.loc[frame[\"Player\"]==\"Pitcher\"]\n",
    "pitcher.to_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")\n",
    "print(len(frame[\"Player\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = frame.loc[frame[\"Player\"]==\"Pitcher\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a.iloc[650, a.columns.get_loc(\"Player\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr1 = ([\"hi\", \"ni\", \"gu\"])\n",
    "arr2 = ([\"hi\", \"nina\", \"gu\"])\n",
    "arr3 = np.asarray(arr1)==np.asarray(arr2)\n",
    "print(arr3)\n",
    "print(np.sum(arr3))\n",
    "\n",
    "print(type(frame[\"Player\"].values[6000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")\n",
    "\n",
    "print(\"csv eingelesen\")\n",
    "# COORDINATES\n",
    "coordinates = cf.iloc[:,cf.columns.get_loc(\"0\"):cf.columns.get_loc(\"166\")]\n",
    "data_array = coordinates.values\n",
    "M, N = data_array.shape\n",
    "data = np.zeros((M,N,18,2))\n",
    "\n",
    "c=0\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        if not pd.isnull(data_array[i,j]):\n",
    "            data[i,j]=np.array(eval(data_array[i,j]))\n",
    "        else:\n",
    "            c+=1\n",
    "\n",
    "np.save(\"coord_array\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEP = 6000\n",
    "EPOCHS = 30\n",
    "\n",
    "print(\"imports\")\n",
    "cf = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")\n",
    "\n",
    "print(\"csv eingelesen\")\n",
    "# COORDINATES\n",
    "\"\"\"\n",
    "coordinates = cf.iloc[:,cf.columns.get_loc(\"0\"):cf.columns.get_loc(\"166\")]\n",
    "data_array = coordinates.values\n",
    "M, N = data_array.shape\n",
    "data = np.zeros((M,N,18,2))\n",
    "\n",
    "c=0\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        if not pd.isnull(data_array[i,j]):\n",
    "            data[i,j]=np.array(eval(data_array[i,j]))\n",
    "        else:\n",
    "            c+=1\n",
    "print(\"nan in daten:\",c)\n",
    "\"\"\"\n",
    "\n",
    "data = np.load(\"coord_array.npy\")\n",
    "M,N, _,_ = data.shape\n",
    "\n",
    "# LABELS\n",
    "def oneHot(dataframe, column):\n",
    "    unique  = np.unique(dataframe[column].values)\n",
    "    l = len(dataframe.index)\n",
    "    loc = dataframe.columns.get_loc(column)\n",
    "    labels = np.zeros((l, 12))\n",
    "    for i in range(l):\n",
    "        #print(cf.iloc[i,loc])\n",
    "        pitch = dataframe.iloc[i,loc]\n",
    "        ind = unique.tolist().index(pitch)\n",
    "        labels[i, ind] = 1\n",
    "    return labels, unique\n",
    "\n",
    "def decode_one_hot(results, unique):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    #unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    for _, pitch in enumerate(results):\n",
    "        ind = np.argmax(pitch)\n",
    "        p.append(unique[ind])\n",
    "    return p\n",
    "\n",
    "labels, unique = oneHot(cf, \"Pitch Type\")\n",
    "label_pitches = (cf[\"Pitch Type\"].values)[SEP:]\n",
    "\n",
    "# NET\n",
    "\n",
    "np.save(\"labels\", labels)\n",
    "np.save(\"unique\", unique)\n",
    "np.save(\"label_pitches\", label_pitches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_16.csv\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[df[\"Player\"] == \"Pitcher\"]\n",
    "coordina = df.iloc[:, df.columns.get_loc(\"0\"):df.columns.get_loc(\"159\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M, N = coordina.values.shape\n",
    "SEP = int(M*0.9)\n",
    "print(\"The training data will be up to %d and the test data from %d to %d\"%(SEP, SEP, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointer = df.columns.get_loc(\"0\")\n",
    "columns = df.columns.tolist()\n",
    "start = pointer\n",
    "while(True):\n",
    "    try:\n",
    "        zahl = int(columns[pointer])\n",
    "        pointer+=1\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "coord = df.iloc[:, start:pointer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing and balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "cf = pd.read_csv(\"merge_16.csv\")\n",
    "cf = cf[cf[\"Player\"]==\"Pitcher\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "print(np.unique(cf[\"Pitch Type\"].values))\n",
    "\n",
    "types = cf[\"Pitch Type\"].values\n",
    "note_frequency = sp.stats.itemfreq(types)\n",
    "print(note_frequency)\n",
    "smaller_20 = (note_frequency[np.where(note_frequency[:,1]<30)])[:,0].flatten()\n",
    "\n",
    "for typ in smaller_20:\n",
    "    cf = cf.drop(cf[cf[\"Pitch Type\"]==typ].index)\n",
    "\n",
    "\n",
    "print(np.unique(cf[\"Pitch Type\"].values))\n",
    "#cf = cf.drop(cf[cf[\"Pitch Type\"]=='Fastball (Split-finger)' or cf[\"Pitch Type\"]==\"Unknown Pitch Type\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.load(\"coord_array.npy\")\n",
    "print(np.any(np.isnan(arr)))\n",
    "means = np.mean(arr, axis = 1)\n",
    "std = np.std(arr, axis = 1)\n",
    "res = np.asarray([(arr[:,i]-means)/(std+0.0001) for i in range(len(arr[0]))])\n",
    "new = np.swapaxes(res, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "data = compute_class_weight(\"auto\", np.unique(cf[\"Pitch Type\"].values),cf[\"Pitch Type\"].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_string= cf[\"Pitch Type\"].values\n",
    "unique =np.unique(labels_string)\n",
    "#print(np.unique(pitches))\n",
    "#print(np.sum(data))\n",
    "nr_classes=len(unique)\n",
    "ex_per_class = 4\n",
    "\n",
    "\"\"\"liste=np.zeros((7,4))\n",
    "for i, types in enumerate(np.unique(pitches)):\n",
    "    liste[i] = (np.random.choice(np.where(pitches==types)[0], 4))\n",
    "print(liste)\n",
    "\"\"\"\n",
    "index_liste = []\n",
    "for pitches in unique:\n",
    "    index_liste.append(np.where(labels_string==pitches))\n",
    "    \n",
    "def balanced_batches(y):   \n",
    "    for j in range(5):\n",
    "        liste=np.zeros((nr_classes, ex_per_class))\n",
    "        for i in range(nr_classes):\n",
    "            liste[i] = np.random.choice(index_liste[i][0], ex_per_class)\n",
    "        #print(liste.flatten().astype(int))\n",
    "        yield y[liste.flatten().astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch_x in balanced_batches(labels_string):\n",
    "    print(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from data_preprocess import Preprocessor\n",
    "\n",
    "def decode_one_hot(results, unique):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    #unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    for _, pitch in enumerate(results):\n",
    "        ind = np.argmax(pitch)\n",
    "        if pitch[ind]>0.5:\n",
    "            p.append(unique[ind])\n",
    "        else:\n",
    "            p.append(\"Too small\")\n",
    "    return p\n",
    "\n",
    "leaky_relu = lambda x: tf.maximum(0.2*x, x)\n",
    "\n",
    "\n",
    "ex_per_class = 4\n",
    "EPOCHS = 10\n",
    "PATH = \"/Users/ninawiedemann/Desktop/UNI/Praktikum/sv_data.csv\"\n",
    "LABELS = \"Pitch Type\"\n",
    "act = leaky_relu\n",
    "CUT_OFF_Classes = 60\n",
    "\n",
    "prepro = Preprocessor(PATH, CUT_OFF_Classes)\n",
    "data = np.load(\"coord_sv.npy\") #prepro.get_coord_arr(\"coord_sv.npy\")\n",
    "\n",
    "M,N,nr_joints,_ = data.shape\n",
    "SEP = int(M*0.9)\n",
    "\n",
    "labels, unique = prepro.get_labels_onehot(LABELS)\n",
    "labels_string = prepro.get_labels(LABELS)\n",
    "#labels_test = decode_one_hot(labels[SEP:, :], unique)\n",
    "\n",
    "nr_classes = len(np.unique(labels_string))\n",
    "BATCHSIZE = nr_classes*ex_per_class\n",
    "print(\"nr classes\", nr_classes, \"Batchsize\", BATCHSIZE)\n",
    "\n",
    "# NET\n",
    "\n",
    "ind = np.random.permutation(len(data))\n",
    "train_ind = ind[:SEP]\n",
    "test_ind = ind[SEP:]\n",
    "\n",
    "train_x = data[train_ind]\n",
    "test_x = data[test_ind]\n",
    "train_t= labels[train_ind]\n",
    "test_t = labels[test_ind]\n",
    "labels_string_train = labels_string[train_ind]\n",
    "labels_string_test = labels_string[test_ind]\n",
    "\n",
    "\"\"\"\n",
    "DATA TESTING:\n",
    "indiuh = np.where(ind==2000)\n",
    "print(\"Labels nach preprocc von 2000\", labels_string[2000])\n",
    "print(\"new Index of 2000\", indiuh, \"test ob where funkt: \", ind[indiuh])\n",
    "print(\"train coord of 2000 u 140\", train_x[indiuh, 140])\n",
    "print(\"labels_string von 2000\", labels_string_train[indiuh])\n",
    "print(\"one hot von 2000\", train_t[indiuh])\n",
    "\"\"\"\n",
    "\n",
    "index_liste = []\n",
    "for pitches in unique:\n",
    "    index_liste.append(np.where(labels_string_train==pitches))\n",
    "\n",
    "len_test = len(test_x)\n",
    "len_train = len(train_x)\n",
    "print(\"Test set size: \", len_test, \" train set size: \", len_train)\n",
    "print(\"Shapes of train_x\", train_x.shape, \"shape of test_x\", test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normal conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, N, nr_joints, 2), name = \"input\")\n",
    "x_ = tf.reshape(x, (-1, N, nr_joints*2))\n",
    "y = tf.placeholder(tf.float32, (None, len(labels[0])))\n",
    "\n",
    "net = tf.layers.conv1d(x_, filters=256, kernel_size=5, strides=2, activation=act)\n",
    "net = tf.layers.conv1d(net, filters=256, kernel_size=3, strides=1, activation=act)\n",
    "net = tf.layers.conv1d(net, filters=128, kernel_size=3, strides=1, activation=act)\n",
    "net = tf.layers.conv1d(net, filters=1, kernel_size=1, activation = act)\n",
    "shapes = net.get_shape().as_list()\n",
    "ff = tf.reshape(net, (-1, shapes[1]*shapes[2]))\n",
    "ff = tf.layers.dense(ff, 1024, activation = act)\n",
    "ff = tf.layers.dense(ff, 128, activation = act)\n",
    "logits = tf.layers.dense(ff, len(labels[0]), activation = None)\n",
    "out = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)) # tf.reduce_mean(tf.square(y-ff))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = BATCHSIZE\n",
    "display_step = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network Parameters\n",
    "n_input = nr_joints*2 # MNIST data input (img shape: 28*28)\n",
    "n_steps = N # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 12 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x_ = tf.placeholder(\"float\", [None, n_steps, nr_joints, 2])\n",
    "x = tf.reshape(x_, (-1, N, nr_joints*2))\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\"\"\"# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def RNN(x):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    \"\"\"with tf.variable_scope(\"myrnn\") as scope:\n",
    "        for i in range(n_steps-1):\n",
    "            if i > 0:\n",
    "                scope.reuse_variables()\n",
    "            output, state = lstm_cell(x[i], state)\"\"\"\n",
    "\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.layers.dense(outputs[-1], n_classes)   #tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "out_logits = RNN(x)\n",
    "out = tf.nn.softmax(out_logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi layer lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = BATCHSIZE\n",
    "display_step = 10\n",
    "nr_layers = 4\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network Parameters\n",
    "n_input = nr_joints*2 # MNIST data input (img shape: 28*28)\n",
    "n_steps = N # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 12 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x_ = tf.placeholder(\"float\", [None, n_steps, nr_joints, 2])\n",
    "x = tf.reshape(x_, (-1, N, nr_joints*2))\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\"\"\"# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def RNN(x):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    #lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    def lstm_cell():\n",
    "          return rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(nr_layers)])\n",
    "\n",
    "\n",
    "    # Get lstm cell output\n",
    "    \"\"\"with tf.variable_scope(\"myrnn\") as scope:\n",
    "        for i in range(n_steps-1):\n",
    "            if i > 0:\n",
    "                scope.reuse_variables()\n",
    "            output, state = lstm_cell(x[i], state)\"\"\"\n",
    "\n",
    "    outputs, states = rnn.static_rnn(stacked_lstm, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.layers.dense(outputs[-1], n_classes)   #tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "out_logits = RNN(x)\n",
    "out = tf.nn.softmax(out_logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batches(x, y, batchsize=32):\n",
    "    permute = np.random.permutation(len(x))\n",
    "    for i in range(0, len(x)-batchsize, batchsize):\n",
    "        indices = permute[i:i+batchsize]\n",
    "        yield x[indices], y[indices]\n",
    "\n",
    "def balanced_batches(x, y, batchsize=32):\n",
    "    for j in range(200):\n",
    "        liste=np.zeros((nr_classes, ex_per_class))\n",
    "        for i in range(nr_classes):\n",
    "            # print(j, i, np.random.choice(index_liste[i][0], ex_per_class))\n",
    "            liste[i] = np.random.choice(index_liste[i][0], ex_per_class, replace=False)\n",
    "        liste = liste.flatten().astype(int)\n",
    "        yield x[liste], y[liste]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run session for EPOCH epochs\n",
    "for epoch in range(EPOCHS + 1):\n",
    "    for batch_x, batch_t in balanced_batches(train_x, train_t, BATCHSIZE):\n",
    "        sess.run(optimizer, {x_: batch_x, y: batch_t})\n",
    "    # print(\"Loss test: \", sess.run(loss, {x: test_x, y: test_t}))\n",
    "    #print(\"Loss train: \", sess.run(loss, {x: train_x, y: train_t}))\n",
    "\n",
    "    #Test Accuracy\n",
    "    loss_test, out_test = sess.run([loss,out], {x_: test_x, y: test_t})\n",
    "    print(\"Loss test\", loss_test)\n",
    "    pitches_test = decode_one_hot(out_test, unique)\n",
    "    print(\"Accuracy test: \", np.sum(np.asarray(labels_string_test)==pitches_test)/len_test)\n",
    "    \n",
    "    #Train Accuracy\n",
    "    out_train = sess.run(out, {x_: train_x, y: train_t})\n",
    "    pitches_train = decode_one_hot(out_train, unique)\n",
    "    print(\"Accuracy train: \", np.sum(np.asarray(labels_string_train)==pitches_train)/SEP)\n",
    "\n",
    "print(pitches_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cut file to stretch and pitch type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "out_list = [\"ine\", \"ine\", \"bdeu\", \"hdba\", \"nina\", \"nina\"]\n",
    "ground_truth_list = [\"ine\",\"ine\", \"bdeu\", \"ine\", \"nina\", \"halo\"]\n",
    "\n",
    "out = np.array(out_list)\n",
    "ground_truth = np.array(ground_truth_list)\n",
    "\n",
    "same = out[np.where(out==ground_truth)[0]]\n",
    "\n",
    "right_frequency = sp.stats.itemfreq(same)\n",
    "total_frequency = sp.stats.itemfreq(ground_truth)\n",
    "right_dict = dict(zip(right_frequency[:,0], right_frequency[:,1]))\n",
    "total_dict = dict(zip(total_frequency[:,0], total_frequency[:,1]))\n",
    "\n",
    "acc= right_dict\n",
    "for types in right_dict.keys():\n",
    "    acc[types] = (int(right_dict[types])/float(total_dict[types]))\n",
    "\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cf = np.load(\"coord_array.npy\")\n",
    "#sv = np.load(\"coord_sv.npy\")\n",
    "#print(cf.shape, sv.shape)\n",
    "\n",
    "cf = pd.read_csv(\"cf_data.csv\")\n",
    "sv = pd.read_csv(\"sv_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = cf[cf[\"Player\"]==\"Pitcher\"]\n",
    "sv = sv[sv[\"Player\"]==\"Pitcher\"]\n",
    "cf_plays = cf['play_id'].values\n",
    "print(sv[sv[\"play_id\"]==cf_plays[0]].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cf_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sv[\"play_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_ids = []\n",
    "sv_plays = sv[\"play_id\"].values\n",
    "for i, ids in enumerate(cf_plays):\n",
    "    if ids in sv_plays:\n",
    "        common_ids.append(ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(common_ids))\n",
    "print(len(sv_plays))\n",
    "print(len(cf_plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cf[\"play_id\"].values[0])\n",
    "new = cf[cf[\"play_id\"] in sv[\"play_id\"].values]\n",
    "print((np.array(sv_plays)==np.array(cf_plays)))\n",
    "print(np.where(np.array(sv_plays)==np.array(cf_plays)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"cf_data.csv\")\n",
    "sv = pd.read_csv(\"sv_data.csv\")\n",
    "\n",
    "\n",
    "cf = cf[cf[\"Player\"]==\"Pitcher\"]\n",
    "sv = sv[sv[\"Player\"]==\"Pitcher\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pitcher = cf[\"Pitcher\"].values.astype(int)\n",
    "statistic = sp.stats.itemfreq(pitcher) #.sort(axis = 0)\n",
    "\n",
    "number = np.array(frequ[:,1])\n",
    "\n",
    "for i in range(5):\n",
    "    maxi = np.argmax(number)\n",
    "    a = frequ[maxi]\n",
    "    number[maxi]=0\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = cf[cf[\"Runner on 1st\"]!=None]\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninawiedemann/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (253,254,255,256,257,258,259,289) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv eingelesen with length  13150\n",
      "Only Pitcher rows\n",
      "([527054.0, 285079.0, 592314.0, 448802.0, 656794.0], array([[  1.12526000e+05,   2.40000000e+02],\n",
      "       [  2.76351000e+05,   4.00000000e+00],\n",
      "       [  2.85079000e+05,   4.35000000e+02],\n",
      "       [  4.07793000e+05,   4.40000000e+01],\n",
      "       [  4.07822000e+05,   1.00000000e+01],\n",
      "       [  4.24144000e+05,   1.00000000e+01],\n",
      "       [  4.30661000e+05,   8.00000000e+00],\n",
      "       [  4.30912000e+05,   4.00000000e+01],\n",
      "       [  4.33217000e+05,   3.00000000e+00],\n",
      "       [  4.34442000e+05,   1.70000000e+01],\n",
      "       [  4.35400000e+05,   1.42000000e+02],\n",
      "       [  4.44468000e+05,   1.10000000e+01],\n",
      "       [  4.46399000e+05,   4.00000000e+00],\n",
      "       [  4.46899000e+05,   1.00000000e+01],\n",
      "       [  4.47714000e+05,   7.70000000e+01],\n",
      "       [  4.48609000e+05,   8.00000000e+00],\n",
      "       [  4.48614000e+05,   8.00000000e+00],\n",
      "       [  4.48802000e+05,   3.25000000e+02],\n",
      "       [  4.50212000e+05,   2.00000000e+00],\n",
      "       [  4.50308000e+05,   2.60000000e+01],\n",
      "       [  4.51584000e+05,   1.10000000e+01],\n",
      "       [  4.52657000e+05,   4.90000000e+01],\n",
      "       [  4.53265000e+05,   6.00000000e+00],\n",
      "       [  4.53281000e+05,   1.30000000e+01],\n",
      "       [  4.53286000e+05,   1.04000000e+02],\n",
      "       [  4.53343000e+05,   7.00000000e+00],\n",
      "       [  4.55009000e+05,   1.10000000e+01],\n",
      "       [  4.56501000e+05,   3.60000000e+01],\n",
      "       [  4.57768000e+05,   9.00000000e+00],\n",
      "       [  4.58006000e+05,   1.60000000e+01],\n",
      "       [  4.58681000e+05,   5.00000000e+01],\n",
      "       [  4.60283000e+05,   2.40000000e+01],\n",
      "       [  4.61829000e+05,   6.00000000e+01],\n",
      "       [  4.62382000e+05,   1.51000000e+02],\n",
      "       [  4.62515000e+05,   5.00000000e+00],\n",
      "       [  4.67008000e+05,   8.00000000e+00],\n",
      "       [  4.67100000e+05,   4.60000000e+01],\n",
      "       [  4.68504000e+05,   4.80000000e+01],\n",
      "       [  4.72610000e+05,   1.70000000e+01],\n",
      "       [  4.73879000e+05,   9.00000000e+00],\n",
      "       [  4.75479000e+05,   2.20000000e+01],\n",
      "       [  4.77569000e+05,   2.70000000e+01],\n",
      "       [  4.88748000e+05,   1.00000000e+01],\n",
      "       [  4.88846000e+05,   7.00000000e+00],\n",
      "       [  4.89334000e+05,   1.60000000e+01],\n",
      "       [  4.90063000e+05,   3.80000000e+01],\n",
      "       [  4.91646000e+05,   1.60000000e+01],\n",
      "       [  4.93157000e+05,   1.10000000e+01],\n",
      "       [  4.93200000e+05,   1.80000000e+01],\n",
      "       [  5.01697000e+05,   1.80000000e+01],\n",
      "       [  5.01789000e+05,   7.00000000e+00],\n",
      "       [  5.01925000e+05,   8.00000000e+00],\n",
      "       [  5.02004000e+05,   1.50000000e+01],\n",
      "       [  5.02046000e+05,   3.40000000e+01],\n",
      "       [  5.02179000e+05,   2.40000000e+01],\n",
      "       [  5.02188000e+05,   3.40000000e+01],\n",
      "       [  5.02190000e+05,   4.40000000e+01],\n",
      "       [  5.02239000e+05,   4.50000000e+01],\n",
      "       [  5.02748000e+05,   4.90000000e+01],\n",
      "       [  5.04379000e+05,   2.00000000e+00],\n",
      "       [  5.17414000e+05,   1.10000000e+01],\n",
      "       [  5.18567000e+05,   9.90000000e+01],\n",
      "       [  5.18693000e+05,   1.59000000e+02],\n",
      "       [  5.18715000e+05,   9.00000000e+00],\n",
      "       [  5.18748000e+05,   8.00000000e+00],\n",
      "       [  5.18774000e+05,   8.90000000e+01],\n",
      "       [  5.18875000e+05,   1.70000000e+01],\n",
      "       [  5.19008000e+05,   9.00000000e+00],\n",
      "       [  5.19043000e+05,   4.60000000e+01],\n",
      "       [  5.19076000e+05,   3.50000000e+01],\n",
      "       [  5.19166000e+05,   1.20000000e+01],\n",
      "       [  5.19294000e+05,   9.00000000e+00],\n",
      "       [  5.19301000e+05,   1.20000000e+01],\n",
      "       [  5.27054000e+05,   4.63000000e+02],\n",
      "       [  5.27055000e+05,   1.00000000e+02],\n",
      "       [  5.34812000e+05,   6.00000000e+00],\n",
      "       [  5.42194000e+05,   3.00000000e+00],\n",
      "       [  5.42432000e+05,   1.41000000e+02],\n",
      "       [  5.43037000e+05,   3.90000000e+01],\n",
      "       [  5.43118000e+05,   1.20000000e+01],\n",
      "       [  5.43272000e+05,   1.00000000e+00],\n",
      "       [  5.43339000e+05,   1.10000000e+01],\n",
      "       [  5.43506000e+05,   9.00000000e+00],\n",
      "       [  5.43557000e+05,   4.40000000e+01],\n",
      "       [  5.43652000e+05,   5.00000000e+00],\n",
      "       [  5.43779000e+05,   1.80000000e+01],\n",
      "       [  5.44727000e+05,   9.00000000e+00],\n",
      "       [  5.44836000e+05,   1.20000000e+01],\n",
      "       [  5.44931000e+05,   5.70000000e+01],\n",
      "       [  5.45332000e+05,   9.00000000e+00],\n",
      "       [  5.45346000e+05,   1.10000000e+01],\n",
      "       [  5.53878000e+05,   1.50000000e+01],\n",
      "       [  5.70257000e+05,   1.30000000e+01],\n",
      "       [  5.70632000e+05,   4.30000000e+01],\n",
      "       [  5.70663000e+05,   5.00000000e+00],\n",
      "       [  5.71521000e+05,   2.70000000e+01],\n",
      "       [  5.71578000e+05,   3.30000000e+01],\n",
      "       [  5.71871000e+05,   1.42000000e+02],\n",
      "       [  5.71882000e+05,   1.20000000e+01],\n",
      "       [  5.71901000e+05,   2.00000000e+00],\n",
      "       [  5.71927000e+05,   4.60000000e+01],\n",
      "       [  5.72096000e+05,   1.20000000e+01],\n",
      "       [  5.72193000e+05,   5.00000000e+00],\n",
      "       [  5.72208000e+05,   6.00000000e+00],\n",
      "       [  5.72831000e+05,   1.20000000e+01],\n",
      "       [  5.73109000e+05,   9.00000000e+00],\n",
      "       [  5.73185000e+05,   3.70000000e+01],\n",
      "       [  5.73186000e+05,   5.10000000e+01],\n",
      "       [  5.91693000e+05,   1.40000000e+01],\n",
      "       [  5.92127000e+05,   1.20000000e+01],\n",
      "       [  5.92130000e+05,   4.00000000e+00],\n",
      "       [  5.92314000e+05,   4.23000000e+02],\n",
      "       [  5.92422000e+05,   2.80000000e+01],\n",
      "       [  5.92426000e+05,   1.19000000e+02],\n",
      "       [  5.92570000e+05,   1.80000000e+01],\n",
      "       [  5.92612000e+05,   7.00000000e+00],\n",
      "       [  5.92665000e+05,   3.50000000e+01],\n",
      "       [  5.92815000e+05,   1.20000000e+01],\n",
      "       [  5.92836000e+05,   1.90000000e+01],\n",
      "       [  5.92866000e+05,   3.70000000e+01],\n",
      "       [  5.93140000e+05,   1.10000000e+01],\n",
      "       [  5.93576000e+05,   4.00000000e+00],\n",
      "       [  5.94798000e+05,   5.60000000e+01],\n",
      "       [  5.94840000e+05,   3.00000000e+00],\n",
      "       [  5.94902000e+05,   3.00000000e+01],\n",
      "       [  5.95014000e+05,   1.70000000e+01],\n",
      "       [  5.95191000e+05,   2.90000000e+01],\n",
      "       [  6.01713000e+05,   4.60000000e+01],\n",
      "       [  6.05151000e+05,   1.10000000e+01],\n",
      "       [  6.05177000e+05,   1.10000000e+01],\n",
      "       [  6.05195000e+05,   1.10000000e+01],\n",
      "       [  6.05200000e+05,   5.80000000e+01],\n",
      "       [  6.05218000e+05,   1.10000000e+01],\n",
      "       [  6.05388000e+05,   1.60000000e+01],\n",
      "       [  6.05397000e+05,   3.00000000e+01],\n",
      "       [  6.05400000e+05,   4.20000000e+01],\n",
      "       [  6.05452000e+05,   4.70000000e+01],\n",
      "       [  6.05538000e+05,   1.13000000e+02],\n",
      "       [  6.06424000e+05,   4.00000000e+00],\n",
      "       [  6.06965000e+05,   1.10000000e+01],\n",
      "       [  6.06983000e+05,   5.00000000e+00],\n",
      "       [  6.07192000e+05,   4.00000000e+01],\n",
      "       [  6.07229000e+05,   1.38000000e+02],\n",
      "       [  6.07352000e+05,   3.10000000e+01],\n",
      "       [  6.07457000e+05,   9.00000000e+00],\n",
      "       [  6.07625000e+05,   4.20000000e+01],\n",
      "       [  6.08379000e+05,   4.20000000e+01],\n",
      "       [  6.08678000e+05,   1.40000000e+01],\n",
      "       [  6.08716000e+05,   9.00000000e+00],\n",
      "       [  6.08718000e+05,   2.10000000e+01],\n",
      "       [  6.21199000e+05,   6.00000000e+00],\n",
      "       [  6.21295000e+05,   5.00000000e+00],\n",
      "       [  6.22766000e+05,   1.40000000e+01],\n",
      "       [  6.23149000e+05,   2.10000000e+01],\n",
      "       [  6.23352000e+05,   9.00000000e+00],\n",
      "       [  6.24586000e+05,   4.00000000e+00],\n",
      "       [  6.43327000e+05,   3.60000000e+01],\n",
      "       [  6.56794000e+05,   2.57000000e+02],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00],\n",
      "       [             nan,   1.00000000e+00]]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from data_preprocess import Preprocessor\n",
    "\n",
    "#pre_cf = Preprocessor(\"cf_data.csv\",60)\n",
    "#new_data = pre_cf.concat_with_second(\"sv_data.csv\")\n",
    "#print(new_data)\n",
    "\n",
    "PATH = \"cf_data.csv\"\n",
    "LABELS = \"Pitch Type\"\n",
    "\n",
    "prepro = Preprocessor(PATH)\n",
    "players = prepro.get_list_with_most(\"Pitcher\")\n",
    "print(players)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Fastball (4-seam)', 'Slider', 'Fastball (2-seam)', 'Curveball', 'Changeup'],\n",
       " array([['Changeup', 29],\n",
       "        ['Curveball', 39],\n",
       "        ['Fastball (2-seam)', 82],\n",
       "        ['Fastball (4-seam)', 226],\n",
       "        ['Slider', 87]], dtype=object))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro.cut_file_to_pitcher(527054.0)\n",
    "prepro.get_list_with_most(\"Pitch Type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cf.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# histogram of 5 players with most pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_preprocess import Preprocessor\n",
    "prepro = Preprocessor(\"cf_data.csv\")\n",
    "players, _ = prepro.get_list_with_most(\"Pitcher\")\n",
    "#print(players[2])\n",
    "pitchi = []\n",
    "for i in range(5):\n",
    "    prepro = Preprocessor(\"cf_data.csv\")\n",
    "    prepro.cut_file_to_pitcher(players[i])\n",
    "    _, stat = prepro.get_list_with_most(\"Pitch Type\")\n",
    "    pitchi.append(stat)\n",
    "print(pitchi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(players[i], \": \", np.sum(pitchi[i][:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data=np.random.random((4,10))\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "titles = players\n",
    "for i in range(5):\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.ylim([0,350])\n",
    "    y_pos = range(len(pitchi[i]))\n",
    "    plt.bar(y_pos, pitchi[i][:,1], align='center', alpha=0.5, width = 1)\n",
    "    #plt.ylim([0,350])\n",
    "    plt.xticks(y_pos, pitchi[i][:,0])\n",
    "    plt.ylabel('number of videos')\n",
    "    plt.title(titles[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to plot coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from data_preprocess import Preprocessor\n",
    "prepro = Preprocessor(\"cf_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players, _ = prepro.get_list_with_most(\"Pitcher\")\n",
    "prepro.select_movement(\"Stretch\")\n",
    "prepro.cut_file_to_pitcher(players[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = prepro.cf\n",
    "pitchi, _ = prepro.get_list_with_most(\"Pitch Type\")\n",
    "df = df[df[\"Pitch Type\"]==pitchi[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepro.cf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = prepro.get_coord_arr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#z=range(167)\n",
    "for i in range(167):\n",
    "    ax.scatter(data[10, i, :16, 0], data[10, i, :16, 1], i, c= 'red', s=4)\n",
    "\n",
    "\n",
    "#for i in range(18):\n",
    "#    ax.scatter(data[10, :, i, 0], data[10, i, :, 1], z, c= 'red', s=1)\n",
    "    \n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "plt.show()\n",
    "#Axes3D.scatter(mean_data[0], mean_data[1], mean_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(max(prepro.cf['first_movement_frame_index'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cf = pd.read_csv(\"cf_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model for one player (here: the one with most trials) and all his pitch types (Stretch&windup mixed):\n",
    "\n",
    "Convolutional NN with 4 conv layers and 3 fully connected feed forward layers, using regularization (L2 loss for weights and Dropout)\n",
    "\n",
    "Frames aligned by first_movement_frame, head coordinates removed (only first 12)\n",
    "\n",
    "nr classes 5 Batchsize 20 trained for 40 epochs, 100 balanced batches per epoch\n",
    "\n",
    "classes:  ['Changeup', 'Curveball', 'Fastball (2-seam)', 'Fastball (4-seam)', 'Slider']\n",
    "\n",
    "data shape: (463, 92, 12, 2) label_shape (463, 5) (463,)\n",
    "\n",
    "Test set size:  47  train set size:  416\n",
    "\n",
    "Shapes of train_x (416, 92, 12, 2) shape of test_x (47, 92, 12, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy test:  0.489361702128\n",
    "Accuracy train:  0.560096153846\n",
    "Loss test 2.0455\n",
    "Accuracy test:  0.531914893617\n",
    "Accuracy train:  0.5625\n",
    "Loss test 2.06014\n",
    "Accuracy test:  0.553191489362\n",
    "Accuracy train:  0.557692307692\n",
    "Loss test 2.04858\n",
    "Accuracy test:  0.489361702128\n",
    "Accuracy train:  0.572115384615\n",
    "Loss test 2.05759\n",
    "Accuracy test:  0.489361702128\n",
    "Accuracy train:  0.591346153846\n",
    "Loss test 2.04937\n",
    "Accuracy test:  0.531914893617\n",
    "Accuracy train:  0.603365384615\n",
    "Loss test 2.08457\n",
    "Accuracy test:  0.553191489362\n",
    "Accuracy train:  0.600961538462\n",
    "Loss test 2.07272\n",
    "Accuracy test:  0.531914893617\n",
    "Accuracy train:  0.622596153846\n",
    "Loss test 2.05784\n",
    "Accuracy test:  0.531914893617\n",
    "Accuracy train:  0.639423076923\n",
    "Loss test 1.99224\n",
    "Accuracy test:  0.574468085106\n",
    "Accuracy train:  0.620192307692\n",
    "True                   Test             ['Changeup', 'Curveball', 'Fastball (2-seam)', 'Fastball (4-seam)', 'Slider']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.50        ', '0.49        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.01        ', '0.01        ', '0.61        ', '0.37        ']\n",
    "Fastball (4-seam)    Slider               ['0.00        ', '0.00        ', '0.00        ', '0.29        ', '0.71        ']\n",
    "Slider               Fastball (2-seam)    ['0.02        ', '0.12        ', '0.40        ', '0.19        ', '0.27        ']\n",
    "Changeup             Fastball (2-seam)    ['0.15        ', '0.23        ', '0.44        ', '0.08        ', '0.09        ']\n",
    "Fastball (4-seam)    Fastball (2-seam)    ['0.14        ', '0.18        ', '0.26        ', '0.17        ', '0.25        ']\n",
    "Fastball (2-seam)    Fastball (2-seam)    ['0.22        ', '0.25        ', '0.29        ', '0.11        ', '0.11        ']\n",
    "Fastball (4-seam)    Fastball (2-seam)    ['0.32        ', '0.09        ', '0.36        ', '0.11        ', '0.12        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.87        ', '0.13        ']\n",
    "Slider               Slider               ['0.07        ', '0.17        ', '0.25        ', '0.19        ', '0.32        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.02        ', '0.01        ', '0.89        ', '0.08        ']\n",
    "Slider               Changeup             ['0.32        ', '0.12        ', '0.32        ', '0.10        ', '0.15        ']\n",
    "Slider               Slider               ['0.00        ', '0.01        ', '0.01        ', '0.44        ', '0.55        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.01        ', '0.00        ', '0.81        ', '0.18        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.01        ', '0.01        ', '0.62        ', '0.36        ']\n",
    "Fastball (4-seam)    Slider               ['0.00        ', '0.01        ', '0.00        ', '0.29        ', '0.70        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.91        ', '0.09        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.74        ', '0.26        ']\n",
    "Fastball (4-seam)    Slider               ['0.09        ', '0.17        ', '0.22        ', '0.18        ', '0.33        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.09        ', '0.00        ', '0.88        ', '0.03        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.93        ', '0.07        ']\n",
    "Fastball (4-seam)    Changeup             ['0.68        ', '0.02        ', '0.27        ', '0.02        ', '0.01        ']\n",
    "Curveball            Curveball            ['0.00        ', '0.87        ', '0.01        ', '0.03        ', '0.09        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.89        ', '0.10        ']\n",
    "Fastball (2-seam)    Fastball (2-seam)    ['0.33        ', '0.08        ', '0.45        ', '0.06        ', '0.08        ']\n",
    "Fastball (4-seam)    Fastball (2-seam)    ['0.04        ', '0.17        ', '0.33        ', '0.18        ', '0.28        ']\n",
    "Fastball (4-seam)    Slider               ['0.12        ', '0.17        ', '0.24        ', '0.17        ', '0.30        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.65        ', '0.34        ']\n",
    "Fastball (4-seam)    Curveball            ['0.03        ', '0.59        ', '0.15        ', '0.10        ', '0.12        ']\n",
    "Changeup             Fastball (2-seam)    ['0.03        ', '0.09        ', '0.53        ', '0.09        ', '0.27        ']\n",
    "Curveball            Curveball            ['0.05        ', '0.80        ', '0.10        ', '0.03        ', '0.02        ']\n",
    "Changeup             Slider               ['0.00        ', '0.32        ', '0.13        ', '0.20        ', '0.34        ']\n",
    "Curveball            Curveball            ['0.00        ', '0.96        ', '0.00        ', '0.01        ', '0.03        ']\n",
    "Slider               Fastball (4-seam)    ['0.00        ', '0.02        ', '0.01        ', '0.57        ', '0.40        ']\n",
    "Fastball (2-seam)    Changeup             ['0.69        ', '0.03        ', '0.21        ', '0.04        ', '0.03        ']\n",
    "Slider               Slider               ['0.00        ', '0.00        ', '0.00        ', '0.44        ', '0.56        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.16        ', '0.03        ', '0.42        ', '0.39        ']\n",
    "Fastball (2-seam)    Fastball (2-seam)    ['0.18        ', '0.21        ', '0.27        ', '0.15        ', '0.19        ']\n",
    "Fastball (4-seam)    Slider               ['0.00        ', '0.02        ', '0.08        ', '0.40        ', '0.50        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.80        ', '0.20        ']\n",
    "Fastball (2-seam)    Changeup             ['0.91        ', '0.00        ', '0.09        ', '0.00        ', '0.00        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.77        ', '0.22        ']\n",
    "Slider               Slider               ['0.09        ', '0.17        ', '0.22        ', '0.18        ', '0.33        ']\n",
    "Slider               Fastball (4-seam)    ['0.00        ', '0.01        ', '0.00        ', '0.68        ', '0.31        ']\n",
    "Slider               Slider               ['0.09        ', '0.17        ', '0.22        ', '0.18        ', '0.33        ']\n",
    "Fastball (4-seam)    Slider               ['0.09        ', '0.17        ', '0.22        ', '0.18        ', '0.33        ']\n",
    "Fastball (4-seam)    Fastball (4-seam)    ['0.00        ', '0.00        ', '0.00        ', '0.67        ', '0.33        ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shorter version of conv net\n",
    "\n",
    "all with alignment and concatenated data\n",
    "\n",
    "conv_short version,      mit max_pooling, with conv-batch_norm-dropout architecture\n",
    "    \n",
    "player0: 53% (5 classes), 46,                  35\n",
    "\n",
    "player1: 80% (2 classes), 85,                  70\n",
    "\n",
    "player2: 27% (6 classes), 21,                  35\n",
    "\n",
    "player3: 45% (5 classes), 23,                  36\n",
    "\n",
    "player4: 35% (4 classes), 52,                  46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same but only one pitchtype:\n",
    "\n",
    "and concat data and alignment with conv-batch_norm-dropout architecture\n",
    "\n",
    "player0: 62\n",
    "\n",
    "player1: 77\n",
    "\n",
    "player2: 75\n",
    "\n",
    "player3: 59\n",
    "\n",
    "player4: 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trained on concatenated data from cf and sv (4 coordinates) on only Windup 53%, only Stretch 54%, all 50%\n",
    "(without alignment and without regularization loss)\n",
    "\"\"\"\n",
    "def best_in_cluster_concat53(self, x, nr_classes, training, rate_dropout=0.6, act=tf.nn.relu):\n",
    "        shape = x.get_shape().as_list()\n",
    "        x_ = tf.reshape(x, (-1, shape[1], shape[2]*shape[3]))\n",
    "        net = tf.layers.conv1d(x_, filters=256, kernel_size=5, strides=2, activation=act, name=\"conv1\")\n",
    "        tf.summary.histogram(\"conv_1_layer\", net)\n",
    "        # net = tf.layers.dropout(net, rate=rate_dropout, training=training)\n",
    "        net = tf.layers.conv1d(net, filters=128, kernel_size=3, activation = act, name=\"conv4\")\n",
    "        shapes = net.get_shape().as_list()\n",
    "        ff = tf.reshape(net, (-1, shapes[1]*shapes[2]))\n",
    "        logits = tf.layers.dense(ff, nr_classes, activation = None, name = \"ff3\")\n",
    "        out = tf.nn.softmax(logits)\n",
    "        return out, logits\n",
    "    \n",
    "\"\"\"Trained on only one pitch type: about 75% with higher batchsize 78\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align pitches by \"first_movement_frame_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "release_frame = prepro.cf[\"first_movement_frame_index\"].values\n",
    "\n",
    "def align_frames(data, release_frame, fr_before, fr_after):\n",
    "    \"\"\"\n",
    "    Takes the data and an array that indicates the frame number of the first movement\n",
    "    cuts all data to fr_before the release_frame and fr_after the release frame\n",
    "    returns an array of size data.shape except the second dimension is cut to length fr_before+fr_after\n",
    "    \"\"\"\n",
    "    M, _, nr_joints, nr_coord = data.shape\n",
    "    new = np.zeros((M, fr_after+fr_before, nr_joints, nr_coord))\n",
    "    for i, row in enumerate(data):\n",
    "        ind = release_frame[i]\n",
    "        if pd.isnull(ind):\n",
    "            ind = np.mean(release_frame[:i])\n",
    "        start = int(ind-fr_before)\n",
    "        end = int(ind+ fr_after)\n",
    "        #print(start, end)\n",
    "        new[i] = data[i, start:end, :,:]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/ LSTM\n",
    "tf learn module to create a RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "def get_network_wide(frames, input_size, num_classes):\n",
    "    \"\"\"Create a one-layer LSTM\"\"\"\n",
    "    net = tflearn.input_data(shape=[None, frames, input_size])\n",
    "    net = tflearn.lstm(net, 256, dropout=0.2)\n",
    "    net = tflearn.fully_connected(net, num_classes, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam',\n",
    "                             loss='categorical_crossentropy', name='output1')\n",
    "    return net\n",
    "model = DNN(get_network_wide(N, nr_joints*nr_coordinates, nr_classes))\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"first_movement_frame_index\")\n",
    "print(prepro.cf[\"first_movement_frame_index\"].values[:50])\n",
    "\n",
    "print(\"'pitch_frame_index'\")\n",
    "print(np.where(prepro.cf['pitch_frame_index'].values<0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on best script so far (54% on all data)\n",
    "\n",
    "regularization best weighted 0.0005\n",
    "\n",
    "dropout made it worse\n",
    "\n",
    "tan also up to 50, leaky relu same\n",
    "\n",
    "aligned made it worse\n",
    "\n",
    "only one conv layer: also 54% but overfitting\n",
    "\n",
    "TRAIN IT ON JUST HEAD COORDINATES: (13:18) 40%, ON JUST ONE COORDINATE(14): 30% --> something wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex = pd.read_csv(\"cf_data.csv\", converters = {'0': eval, '1': eval, '2': eval, '3': eval, '4': eval, '5': eval, '6': eval, '7': eval, '8': eval, '9': eval, '10': eval, '11': eval, '12': eval, '13': eval, '14': eval, '15': eval, '16': eval, '17': eval, '18': eval, '19': eval, '20': eval, '21': eval, '22': eval, '23': eval, '24': eval, '25': eval, '26': eval, '27': eval, '28': eval, '29': eval, '30': eval, '31': eval, '32': eval, '33': eval, '34': eval, '35': eval, '36': eval, '37': eval, '38': eval, '39': eval, '40': eval, '41': eval, '42': eval, '43': eval, '44': eval, '45': eval, '46': eval, '47': eval, '48': eval, '49': eval, '50': eval, '51': eval, '52': eval, '53': eval, '54': eval, '55': eval, '56': eval, '57': eval, '58': eval, '59': eval, '60': eval, '61': eval, '62': eval, '63': eval, '64': eval, '65': eval, '66': eval, '67': eval, '68': eval, '69': eval, '70': eval, '71': eval, '72': eval, '73': eval, '74': eval, '75': eval, '76': eval, '77': eval, '78': eval, '79': eval, '80': eval, '81': eval, '82': eval, '83': eval, '84': eval, '85': eval, '86': eval, '87': eval, '88': eval, '89': eval, '90': eval, '91': eval, '92': eval, '93': eval, '94': eval, '95': eval, '96': eval, '97': eval, '98': eval, '99': eval, '100': eval, '101': eval, '102': eval, '103': eval, '104': eval, '105': eval, '106': eval, '107': eval, '108': eval, '109': eval, '110': eval, '111': eval, '112': eval, '113': eval, '114': eval, '115': eval, '116': eval, '117': eval, '118': eval, '119': eval, '120': eval, '121': eval, '122': eval, '123': eval, '124': eval, '125': eval, '126': eval, '127': eval, '128': eval, '129': eval, '130': eval, '131': eval, '132': eval, '133': eval, '134': eval, '135': eval, '136': eval, '137': eval, '138': eval, '139': eval, '140': eval, '141': eval, '142': eval, '143': eval, '144': eval, '145': eval, '146': eval, '147': eval, '148': eval, '149': eval, '150': eval, '151': eval, '152': eval, '153': eval, '154': eval, '155': eval, '156': eval, '157': eval, '158': eval, '159': eval, '160': eval, '161': eval, '162': eval, '163': eval, '164': eval, '165': eval, '166': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for i in range(167):\n",
    "    sys.stdout.write(\"'\"+str(i)+\"': eval, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df ={\"Batchsize\": 40., \"Epochs\": 50., \"batch_nr_in_epoch\": 100., \"file\": \"cf\", \"act\": tf.nn.relu,\n",
    "                   \"CUT_OFF_Classes\": 50., \"dropout\": 0.6, \"learning_rate\": 0.0001, \"lstm_units\": 4., \n",
    "                   \"lstm_hidden_layers\": 128.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df =pd.DataFrame({\"Batchsize\": pd.Series(40.), \"Epochs\": pd.Series(50.), \"batch_nr_in_epoch\": pd.Series(100.), \n",
    "                  \"file\": pd.Series(\"cf\"), \"act\": pd.Series(tf.nn.relu), \"nr_joints\": pd.Series(12),\n",
    "                   \"CUT_OFF_Classes\": pd.Series(50.), \"dropout\": pd.Series(0.6), \"learning_rate\": pd.Series(0.0001),\n",
    "                  \"lstm_units\": pd.Series(4.),\"lstm_hidden_layers\": pd.Series(128.), \"loss\": pd.Series([[5,4,3,2,1]]),\n",
    "                 \"test_acc\": pd.Series([[3,4,5,6]]), \"highest_acc\": pd.Series(10), \"nr_classes\": pd.Series(5), \"align\": pd.Series(True)\n",
    "                 , \"train_acc\": pd.Series([[3,4,5]]), \"len_train\": pd.Series(2000), \"model\":pd.Series(\"conv2d\")})\n",
    "df.to_csv(\"test_parameters.csv\")\n",
    "#{\"Batchsize\": 40., \"Epochs\": 50.}\n",
    "#df[\"Batchsize\"]=pd.Series(40)\n",
    "#df[\"Epoch\"]=10\n",
    "#a = pd.DataFrame.from_dict(df, orient = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = pd.read_csv(\"test_parameters.csv\")\n",
    "columns = new.columns.values.tolist()\n",
    "print(len(columns))\n",
    "#add = pd.DataFrame([[0, 20,30 , 12, 60, tf.nn.tanh,[1,2,3], 0.4, \"sv\", 0.01, 256, 5]], columns=columns)\n",
    "#print(add)\n",
    "#concat = new.append(add, ignore_index = True)\n",
    "#print(new)\n",
    "#print(concat[\"coordinates_used\"])\n",
    "#cut = new.drop(new.columns[[0]], axis=1, inplace=True)\n",
    "#print(cut.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = pd.read_csv(\"test_parameters.csv\")\n",
    "columns = new.columns.values.tolist()\n",
    "add = pd.DataFrame([[0, BATCHSIZE, CUT_OFF_Classes , EPOCHS, act, align, batch_nr_in_epoch, rate_dropout\n",
    "                     , PATH, max(acc_test), learning_rate, loss, n_hidden, nr_layers, nr_classes, nr_joints, test_acc, train_acc]], columns=columns)\n",
    "concat = new.append(add, ignore_index = True)\n",
    "concat.to_csv(\"new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop one row of csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inde = pd.read_csv(\"test_parameters.csv\")\n",
    "a = inde.drop(inde.columns[[0]], axis=1, inplace=True)\n",
    "#print(a[\"Epochs\"])\n",
    "a.to_csv(\"test_parameters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 80\n",
    "batch_nr_in_epoch = 100\n",
    "align = False\n",
    "act = tf.nn.relu\n",
    "rate_dropout = 0\n",
    "learning_rate = 0.0005\n",
    "nr_layers = 4\n",
    "n_hidden = 128\n",
    "BATCHSIZE = 40\n",
    "network = \"conv1d (256, 5) - conv1d(128, 3) - dense(nr_classes) - softmax\"\n",
    "CUT_OFF_Classes = 50\n",
    "PATH = \"sv\"\n",
    "len_train = 5605\n",
    "losses= [6,5,4]\n",
    "nr_classes = 9\n",
    "nr_joints =12\n",
    "acc_train = [1,2,3]\n",
    "acc_test = [2,3,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = pd.read_csv(\"test_parameters_testing.csv\")\n",
    "columns = new.columns.values.tolist()# .append(\"optimizer\")\n",
    "    # print(len(columns))\n",
    "    # print(\"Written to csv: \", BATCHSIZE, CUT_OFF_Classes , EPOCHS, act, align, batch_nr_in_epoch, rate_dropout\n",
    "    #                      , PATH, max(acc_test), learning_rate, len_train, losses, n_hidden, nr_layers, network, nr_classes,\n",
    "    #                      nr_joints, acc_train, acc_test)\n",
    "\n",
    "add = pd.DataFrame([[0, BATCHSIZE, CUT_OFF_Classes , EPOCHS, act, align, batch_nr_in_epoch, rate_dropout\n",
    "                     , PATH, max(acc_test), learning_rate, len_train, losses, n_hidden, nr_layers, network, nr_classes,\n",
    "                     nr_joints, acc_train, acc_test, \"sgd\", \"dhe\"]], columns=columns)\n",
    "concat = new.append(add, ignore_index = True)\n",
    "concat[\"optimizer\"]= \"sgd\"\n",
    "concat.drop(concat.columns[[0]], axis=1, inplace=True)\n",
    "concat.to_csv(\"test_parameters_testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_parameters.csv\")\n",
    "columns = df.columns.values.tolist()\n",
    "print(eval(columns))\n",
    "# df = df.drop(df.index[[0,1,2,3,4,5]])\n",
    "# df.drop(df.columns[[0,1,2,3]], axis=1, inplace=True)\n",
    "#df[\"1st_hidden_dense\"]= pd.Series(1024)\n",
    "#df[\"1st_hidden_dense\"]= 1024\n",
    "#df[\"2nd_hidden_dense\"]= pd.Series(128)\n",
    "#df[\"2nd_hidden_dense\"]= 128\n",
    "#df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "#df.to_csv(\"test_parameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"beispiel.csv\")\n",
    "cf = pd.read_csv(\"cf_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"cf_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.stats\n",
    "test = cf[\"Game\"].values\n",
    "#print(np.unique(test))\n",
    "print(max(sp.stats.itemfreq(test)[:, 1]))\n",
    "#print(cf.iloc[3086], cf.iloc[9464])\n",
    "#print(len(cf))\n",
    "#print(len(cf.iloc[3086].values))\n",
    "eins = cf.iloc[3086].values\n",
    "zwei = cf.iloc[9464].values\n",
    "print(test[3086], test[9464])\n",
    "for i in range(len(eins)):\n",
    "    if eins[i]!=zwei[i]:\n",
    "        print(cf.columns.tolist()[i], eins[i], zwei[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "directory = \"/Users/ninawiedemann/Desktop/UNI/Praktikum/output/\"\n",
    "list_files = listdir(directory)\n",
    "start_frames = cf.columns.get_loc(\"0\")\n",
    "#print(list_files)\n",
    "for file in list_files:\n",
    "    if file[0]!='.':\n",
    "        csv = pd.read_csv(directory+file)\n",
    "        #print(csv[\"Unnamed: 0\"].values)\n",
    "        game_id = csv[csv[\"Frame\"]==\"Game\"][\"Pitcher_player\"].values[0]\n",
    "        last_frame = eval(csv[csv[\"Frame\"]==\"# of last frame\"][\"Pitcher_player\"].values[0])+1\n",
    "        print(game_id, last_frame)\n",
    "        #ind = cf[cf[\"play_id\"]==game_id].index.values\n",
    "        #print(ind)\n",
    "\n",
    "        #print(cf[\"play_id\"].values)\n",
    "        #print(cf.columns.tolist())\n",
    "        \n",
    "        location_play = cf[cf[\"Game\"]==game_id].index.values\n",
    "        print(location_play)\n",
    "        if len(location_play)==4:\n",
    "            cf.drop(cf.index[[location_play[2], location_play[3]]])\n",
    "        # get index of play_id in cf\n",
    "        for i in range(last_frame):\n",
    "            pitcher_arr = csv[\"Pitcher_player\"].values[i].replace('\\n', ',').replace('[  ', '[').replace(\"  \", \",\").replace(\",,\",\",\")\n",
    "            batter_arr = csv[\"Batter_player\"].values[i].replace('\\n', ',').replace('[  ', '[').replace(\"  \", \",\").replace(\",,\",\",\")\n",
    "            \n",
    "            cf.iloc[int(location_play[0]), start_frames+i] = pitcher_arr #csv[\"Pitcher_player\"].values[i]\n",
    "            cf.iloc[int(location_play[1]), start_frames+i] = batter_arr #csv[\"Batter_player\"].values[i]\n",
    "            # check if right order of pitcher and batter\n",
    "\n",
    "cf.to_csv(\"unprocessed_data.csv\")\n",
    "\n",
    "print(len(cf.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unpro = pd.read_csv(\"unprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = unpro[unpro[\"Game\"]==\"490780-7be4d62a-6ddb-41e2-9331-dd9656df1819\"] #.index.values\n",
    "#print(lines)\n",
    "bsp  = lines[\"162\"].values[1]\n",
    "# print(repr(bsp))\n",
    "#besp = bsp.replace('\\n', ',').replace('[  ', '[').replace(\"  \", \",\").replace(\",,\",\",\")\n",
    "#print(besp)\n",
    "#import ast\n",
    "print(eval(bsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cf_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = df[df[\"Game\"]==\"490780-7be4d62a-6ddb-41e2-9331-dd9656df1819\"]\n",
    "bsp1 = lines[\"162\"].values[1]\n",
    "print(bsp1)\n",
    "print(eval(bsp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = unpro[\"Game\"].values\n",
    "#print(np.unique(test))\n",
    "print(max(sp.stats.itemfreq(test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_preprocess import Preprocessor\n",
    "pre = Preprocessor(\"unprocessed_data.csv\")\n",
    "pre.remove_small_classes(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.0 0.0\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"unpro_all_coord.npy\")\n",
    "print(np.amax(data), np.amin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M,N, nr_j, _ = data.shape\n",
    "#flat = np.flatten()\n",
    "#print(np.where(data[:,:,:, 0].flatten()==0 and data[:,:,:, 1]!=0))\n",
    "print(data.shape)\n",
    "data = data[:,:,:12,:]\n",
    "\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        for k in range(12):\n",
    "            if k!=16 and data[i,j,k,0]==0:\n",
    "                if j<150:\n",
    "                    print(i,j,k)\n",
    "                ind  = np.where(data[i, :, k, 0]>0)[0]\n",
    "                if ind[-1]>j and ind[0]<j: #ind!=np.array([]) and \n",
    "                    #print(ind)\n",
    "                    ind_before = ind[ind<j][-1]\n",
    "                    ind_after = ind[ind>j][0]\n",
    "                    inter0 = np.linspace(data[i, ind_before, k, 0], data[i, ind_after, k, 0], ind_after-ind_before, endpoint = False)\n",
    "                    inter1 = np.linspace(data[i, ind_before, k, 1], data[i, ind_after, k, 1], ind_after-ind_before, endpoint = False)\n",
    "                    #print(ind)\n",
    "                    #print(i,j,k)\n",
    "                    #print(ind)\n",
    "                    #print(data[i,j])\n",
    "                    #print(data[i, j:ind_after, k, 0])\n",
    "                    data[i,j:ind_after,k,0]= inter0[1:]\n",
    "                    data[i,j:ind_after,k,1]= inter1[1:]\n",
    "                    #print(data[i,j])\n",
    "                elif ind[-1]>j:\n",
    "                    ind_before = ind[ind>j][0]  \n",
    "                    data[i,j,k,0] = data[i,ind_before,k,0]\n",
    "                    data[i,j,k,1] = data[i,ind_before,k,1]\n",
    "                elif ind[0]<j:\n",
    "                    ind_after = ind[ind<j][-1]\n",
    "                    data[i,j,k,0] = data[i,ind_after,k,0]\n",
    "                    data[i,j,k,1] = data[i,ind_after,k,1]\n",
    "                    \n",
    "print(data.shape)\n",
    "print(data)\n",
    "np.save(\"interpolated.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitness = [(524, ['relu', 0.0, True, 0.001, 0.0050000000000000001, 1024, 5, 512, 7, 0, 256]), (658, ['relu', 0.59999999999999998, False, 0.00050000000000000001, 0.00050000000000000001, 512, 7, 512, 11, 128, 0]), (146, ['leaky', 0.59999999999999998, False, 0.00050000000000000001, 0.0, 128, 11, 0, 7, 128, 256]), (390, ['leaky', 0.59999999999999998, False, 0.00050000000000000001, 0.0, 128, 3, 256, 3, 128, 0]), (12, ['relu', 0.59999999999999998, False, 0.00050000000000000001, 0.0050000000000000001, 256, 3, 0, 9, 0, 256]), (270, ['leaky', 0.59999999999999998, True, 0.00025000000000000001, 0.0, 0, 5, 128, 9, 128, 256]), (404, ['leaky', 0.0, False, 0.00050000000000000001, 0.00050000000000000001, 128, 11, 128, 9, 256, 128]), (1170, ['relu', 0.0, True, 0.00050000000000000001, 0.00050000000000000001, 512, 9, 128, 9, 1024, 0]), (650, ['leaky', 0.0, False, 0.0001, 0.0, 512, 5, 512, 5, 128, 128]), (1036, ['relu', 0.0, False, 0.001, 0.001, 128, 7, 0, 5, 1024, 1024])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value by ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on prednet (https://github.com/coxlab/prednet) but code did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"interpolated.npy\")\n",
    "print(data.shape)\n",
    "\n",
    "source = np.zeros((6422*167))\n",
    "for i in range(6422):\n",
    "    source[i*167:(i+1)*167] = i\n",
    "\n",
    "data_images = np.reshape(data, (6422*167, 12,2))\n",
    "\n",
    "split1 = 6000*167\n",
    "split2 = 6400*167\n",
    "data_train = data_images[:split1]\n",
    "data_val = data_images[split1:split2]\n",
    "data_test = data_images[split2:]\n",
    "sources_train = source[:split1]\n",
    "sources_val = source[split1:split2]\n",
    "sources_test = source[split2:]\n",
    "\n",
    "print(data_train.shape, data_val.shape, data_test.shape, sources_train.shape, sources_val.shape, sources_test.shape)\n",
    "\n",
    "np.save(\"X_train.npy\", data_train)\n",
    "np.save(\"X_val.npy\", data_val)\n",
    "np.save(\"X_test.npy\", data_test)\n",
    "np.save(\"sources_train.npy\", sources_train)\n",
    "np.save(\"sources_val.npy\", sources_val)\n",
    "np.save(\"sources_test.npy\", sources_test)\n",
    "\n",
    "train_file = np.expand_dims(np.load(\"DATA_DIR/X_train.npy\"), axis = 3)\n",
    "train_sources = np.expand_dims(np.load(\"DATA_DIR/sources_train.npy\"), axis = 1) #os.path.join(DATA_DIR, 'sources_train.hkl\n",
    "val_file = np.expand_dims(np.load(\"DATA_DIR/X_val.npy\"), axis = 3) #os.path.join(DATA_DIR, 'X_val.hkl')\n",
    "val_sources = np.expand_dims(np.load(\"DATA_DIR/sources_val.npy\"), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
