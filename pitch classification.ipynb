{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practise Pandas and clean merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_16.csv\")\n",
    "# print(file.columns.tolist())\n",
    "# print(file[\"Extension (P)\"][4])\n",
    "# file.dtypes()\n",
    "# print(file.head())\n",
    "# f체r array nur von den Daten: file.values()\n",
    "# file.sort_index(axis = 1, ascending = True) //um 체berschriften zu sortieren zb nach alphabet\n",
    "#sorted = file.sort_values(by = \"Extension (P)\")\n",
    "# print(a[\"Extension (P)\"])\n",
    "# f체r statistics also mean std usw: file.describe()\n",
    "# print(file[0:3])\n",
    "# mehrere spalten ausw채hlen: file.loc[:, ['A','B']]\n",
    "\n",
    "\n",
    "# a= df['Extension (P)'].values\n",
    "# print(a)\n",
    "# print(np.any(np.isnan(a)))\n",
    "\n",
    "df = df.sort_values(by = \"Backspin Rate (P)\")\n",
    "print(df[\"Backspin Rate (P)\"])\n",
    "for i in range(2289,2269, -1):\n",
    "    df = df.drop(df.index[[i]])\n",
    "print(df[\"Backspin Rate (P)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "koord = df.iloc[:,df.columns.get_loc(\"0\"):df.columns.get_loc(\"159\")]\n",
    "\n",
    "\"\"\"clean data by deleting nan columns\"\"\"\n",
    "for col in df.columns.tolist():\n",
    "    if (type(df[col][0]) is not np.float64 and type(df[col][0]) is not np.int64) or np.any(np.isnan(df[col].values)):\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "for col in koord.columns.tolist():\n",
    "    df[col]=koord[col]\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_clean.csv\")\n",
    "#print(cl.columns.tolist())\n",
    "#print(cl[\"0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_data.csv\")\n",
    "sv = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/sv_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get coordinate values and pitch type labels seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coordinates = cf.iloc[:,cf.columns.get_loc(\"0\"):cf.columns.get_loc(\"166\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique  = np.unique(cf[\"Pitch Type\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneHot(dataframe, column):\n",
    "    unique  = np.unique(dataframe[column].values)\n",
    "    l = len(dataframe.index)\n",
    "    loc = dataframe.columns.get_loc(column)\n",
    "    labels = np.zeros((l, 12))\n",
    "    for i in range(l):\n",
    "        #print(cf.iloc[i,loc])\n",
    "        pitch = dataframe.iloc[i,loc]\n",
    "        ind = unique.tolist().index(pitch)\n",
    "        labels[i, ind] = 1\n",
    "    return labels, unique\n",
    "\n",
    "labels, _ = oneHot(cf, \"Pitch Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_array = coordinates.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change pandas dataframe to np.array\n",
    "(Problem: missing values in den Koordinaten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M, N = data_array.shape\n",
    "data = np.zeros((M,N,18,2))\n",
    "\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        if not pd.isnull(data_array[i,j]):\n",
    "            data[i,j]=np.array(eval(data_array[i,j]))\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing = pd.isnull(data_array)\n",
    "print(\"Ratio missing: \", np.count_nonzero(missing)/(M*N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = data[:13000,:]\n",
    "test_x = data[13000:, :]\n",
    "train_t= labels[:13000,:]\n",
    "test_t = labels[13000:, :]\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, N, 18, 2), name = \"input\")\n",
    "x_ = tf.reshape(x, (-1, N, 36 ,1))\n",
    "y = tf.placeholder(tf.float32, (None, len(labels[0])))\n",
    "\n",
    "net = tf.layers.conv2d(x_, filters=16, kernel_size=5, strides=2, activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=16, kernel_size=3, strides=1, activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=32, kernel_size=3, strides=1, activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=1, kernel_size=1)\n",
    "shapes = net.get_shape().as_list()\n",
    "ff = tf.reshape(net, (-1, shapes[1]*shapes[2]))\n",
    "ff = tf.layers.dense(ff, 128, activation = tf.nn.relu)\n",
    "ff = tf.layers.dense(ff, len(labels[0]), activation = tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y-ff))\n",
    "optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batches(x, y, batchsize=32):\n",
    "    permute = np.random.permutation(len(x))\n",
    "    for i in range(0, len(x)-batchsize, batchsize):\n",
    "        indices = permute[i:i+batchsize]\n",
    "        yield x[indices], y[indices]\n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run session for 2000 epochs\n",
    "for epoch in range(2000 + 1):\n",
    "    for batch_x, batch_t in batches(train_x, train_t, 32):\n",
    "        sess.run(optimizer, {x: batch_x, y: batch_t})\n",
    "    print(epoch, sess.run(loss, {x: test_x, y: test_t}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sess.run(loss, {x: test_x, y: test_t}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode_one_hot(results):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    for pitch, i in enumerate(results):\n",
    "        ind = np.argmax(pitch)\n",
    "        p.append(unique[ind])\n",
    "    return p\n",
    "\n",
    "testing = sess.run(ff, {x: test_x, y: test_t})\n",
    "pitches = decode_one_hot(testing)\n",
    "label_pitches = (cf[\"Pitch Type\"].values)[13000:]\n",
    "\n",
    "# evaluate error rate\n",
    "right = 0\n",
    "wrong = 0\n",
    "for i in range(len(pitches)):\n",
    "    if pitches[i]==label_pitches[i]:\n",
    "        right+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "print(right/(right+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(dataframe, column):\n",
    "    unique  = np.unique(dataframe[column].values)\n",
    "    l = len(dataframe.index)\n",
    "    loc = dataframe.columns.get_loc(column)\n",
    "    labels = np.zeros((l, 12))\n",
    "    for i in range(l):\n",
    "        #print(cf.iloc[i,loc])\n",
    "        pitch = dataframe.iloc[i,loc]\n",
    "        ind = unique.tolist().index(pitch)\n",
    "        labels[i, ind] = 1\n",
    "    return labels, unique\n",
    "\n",
    "def decode_one_hot(results, unique):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    #unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    print(results)\n",
    "    for i, pitch in enumerate(results):\n",
    "        #print(pitch)\n",
    "        ind = np.argmax(pitch)\n",
    "        #print(ind)\n",
    "        p.append(unique[ind])\n",
    "    return p\n",
    "\n",
    "labels, unique = oneHot(frame, \"Pitch Type\")\n",
    "\n",
    "new = decode_one_hot(labels, unique)\n",
    "print(np.sum(new==frame[\"Pitch Type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only pitcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_data.csv\")\n",
    "#df = df.sort_values(by = \"Player\") # Ball Picther or Strike outcome pitcher\n",
    "# bis 8615 ball/pitcher\n",
    "pitcher = df.loc[frame[\"Player\"]==\"Pitcher\"]\n",
    "pitcher.to_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")\n",
    "print(len(frame[\"Player\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = frame.loc[frame[\"Player\"]==\"Pitcher\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a.iloc[650, a.columns.get_loc(\"Player\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr1 = ([\"hi\", \"ni\", \"gu\"])\n",
    "arr2 = ([\"hi\", \"nina\", \"gu\"])\n",
    "arr3 = np.asarray(arr1)==np.asarray(arr2)\n",
    "print(arr3)\n",
    "print(np.sum(arr3))\n",
    "\n",
    "print(type(frame[\"Player\"].values[6000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")\n",
    "\n",
    "print(\"csv eingelesen\")\n",
    "# COORDINATES\n",
    "coordinates = cf.iloc[:,cf.columns.get_loc(\"0\"):cf.columns.get_loc(\"166\")]\n",
    "data_array = coordinates.values\n",
    "M, N = data_array.shape\n",
    "data = np.zeros((M,N,18,2))\n",
    "\n",
    "c=0\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        if not pd.isnull(data_array[i,j]):\n",
    "            data[i,j]=np.array(eval(data_array[i,j]))\n",
    "        else:\n",
    "            c+=1\n",
    "\n",
    "np.save(\"coord_array\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEP = 6000\n",
    "EPOCHS = 30\n",
    "\n",
    "print(\"imports\")\n",
    "cf = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/cf_only_pitcher.csv\")\n",
    "\n",
    "print(\"csv eingelesen\")\n",
    "# COORDINATES\n",
    "\"\"\"\n",
    "coordinates = cf.iloc[:,cf.columns.get_loc(\"0\"):cf.columns.get_loc(\"166\")]\n",
    "data_array = coordinates.values\n",
    "M, N = data_array.shape\n",
    "data = np.zeros((M,N,18,2))\n",
    "\n",
    "c=0\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        if not pd.isnull(data_array[i,j]):\n",
    "            data[i,j]=np.array(eval(data_array[i,j]))\n",
    "        else:\n",
    "            c+=1\n",
    "print(\"nan in daten:\",c)\n",
    "\"\"\"\n",
    "\n",
    "data = np.load(\"coord_array.npy\")\n",
    "M,N, _,_ = data.shape\n",
    "\n",
    "# LABELS\n",
    "def oneHot(dataframe, column):\n",
    "    unique  = np.unique(dataframe[column].values)\n",
    "    l = len(dataframe.index)\n",
    "    loc = dataframe.columns.get_loc(column)\n",
    "    labels = np.zeros((l, 12))\n",
    "    for i in range(l):\n",
    "        #print(cf.iloc[i,loc])\n",
    "        pitch = dataframe.iloc[i,loc]\n",
    "        ind = unique.tolist().index(pitch)\n",
    "        labels[i, ind] = 1\n",
    "    return labels, unique\n",
    "\n",
    "def decode_one_hot(results, unique):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    #unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    for _, pitch in enumerate(results):\n",
    "        ind = np.argmax(pitch)\n",
    "        p.append(unique[ind])\n",
    "    return p\n",
    "\n",
    "labels, unique = oneHot(cf, \"Pitch Type\")\n",
    "label_pitches = (cf[\"Pitch Type\"].values)[SEP:]\n",
    "\n",
    "# NET\n",
    "\n",
    "np.save(\"labels\", labels)\n",
    "np.save(\"unique\", unique)\n",
    "np.save(\"label_pitches\", label_pitches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/merge_16.csv\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[df[\"Player\"] == \"Pitcher\"]\n",
    "coordina = df.iloc[:, df.columns.get_loc(\"0\"):df.columns.get_loc(\"159\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M, N = coordina.values.shape\n",
    "SEP = int(M*0.9)\n",
    "print(\"The training data will be up to %d and the test data from %d to %d\"%(SEP, SEP, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointer = df.columns.get_loc(\"0\")\n",
    "columns = df.columns.tolist()\n",
    "start = pointer\n",
    "while(True):\n",
    "    try:\n",
    "        zahl = int(columns[pointer])\n",
    "        pointer+=1\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "coord = df.iloc[:, start:pointer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing and balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "cf = pd.read_csv(\"merge_16.csv\")\n",
    "cf = cf[cf[\"Player\"]==\"Pitcher\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "print(np.unique(cf[\"Pitch Type\"].values))\n",
    "\n",
    "types = cf[\"Pitch Type\"].values\n",
    "note_frequency = sp.stats.itemfreq(types)\n",
    "print(note_frequency)\n",
    "smaller_20 = (note_frequency[np.where(note_frequency[:,1]<30)])[:,0].flatten()\n",
    "\n",
    "for typ in smaller_20:\n",
    "    cf = cf.drop(cf[cf[\"Pitch Type\"]==typ].index)\n",
    "\n",
    "\n",
    "print(np.unique(cf[\"Pitch Type\"].values))\n",
    "#cf = cf.drop(cf[cf[\"Pitch Type\"]=='Fastball (Split-finger)' or cf[\"Pitch Type\"]==\"Unknown Pitch Type\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.load(\"coord_array.npy\")\n",
    "print(np.any(np.isnan(arr)))\n",
    "means = np.mean(arr, axis = 1)\n",
    "std = np.std(arr, axis = 1)\n",
    "res = np.asarray([(arr[:,i]-means)/(std+0.0001) for i in range(len(arr[0]))])\n",
    "new = np.swapaxes(res, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "data = compute_class_weight(\"auto\", np.unique(cf[\"Pitch Type\"].values),cf[\"Pitch Type\"].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_string= cf[\"Pitch Type\"].values\n",
    "unique =np.unique(labels_string)\n",
    "#print(np.unique(pitches))\n",
    "#print(np.sum(data))\n",
    "nr_classes=len(unique)\n",
    "ex_per_class = 4\n",
    "\n",
    "\"\"\"liste=np.zeros((7,4))\n",
    "for i, types in enumerate(np.unique(pitches)):\n",
    "    liste[i] = (np.random.choice(np.where(pitches==types)[0], 4))\n",
    "print(liste)\n",
    "\"\"\"\n",
    "index_liste = []\n",
    "for pitches in unique:\n",
    "    index_liste.append(np.where(labels_string==pitches))\n",
    "    \n",
    "def balanced_batches(y):   \n",
    "    for j in range(5):\n",
    "        liste=np.zeros((nr_classes, ex_per_class))\n",
    "        for i in range(nr_classes):\n",
    "            liste[i] = np.random.choice(index_liste[i][0], ex_per_class)\n",
    "        #print(liste.flatten().astype(int))\n",
    "        yield y[liste.flatten().astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch_x in balanced_batches(labels_string):\n",
    "    print(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninawiedemann/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1167,1168,1169,1170,1171,1172,1173,1203) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv eingelesen with length  9082\n",
      "Only Pitcher rows\n",
      "[['Changeup' 363]\n",
      " ['Curveball' 428]\n",
      " ['Eephus' 1]\n",
      " ['Fastball (2-seam)' 960]\n",
      " ['Fastball (4-seam)' 1451]\n",
      " ['Fastball (Cut)' 191]\n",
      " ['Fastball (Split-finger)' 33]\n",
      " ['Knuckle curve' 54]\n",
      " ['Knuckleball' 214]\n",
      " ['Sinker' 181]\n",
      " ['Slider' 662]\n",
      " ['Unknown Pitch Type' 3]]\n",
      "Removed because not enought class members:  ['Eephus' 'Fastball (Split-finger)' 'Knuckle curve' 'Unknown Pitch Type']\n",
      "nr classes 8 Batchsize 32\n",
      "Test set size:  445  train set size:  4005\n",
      "Shapes of train_x (4005, 249, 18, 2) shape of test_x (445, 249, 18, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from data_preprocess import Preprocessor\n",
    "\n",
    "def decode_one_hot(results, unique):\n",
    "    \"\"\"takes the maximum value and gets the corresponding pitch type\n",
    "    input: array of size trials * pitchTypesNr\n",
    "    returns: array of size trials containing the pitch type as a string\n",
    "    \"\"\"\n",
    "    #unique  = np.unique(cf[\"Pitch Type\"].values)\n",
    "    p = []\n",
    "    for _, pitch in enumerate(results):\n",
    "        ind = np.argmax(pitch)\n",
    "        if pitch[ind]>0.5:\n",
    "            p.append(unique[ind])\n",
    "        else:\n",
    "            p.append(\"Too small\")\n",
    "    return p\n",
    "\n",
    "leaky_relu = lambda x: tf.maximum(0.2*x, x)\n",
    "\n",
    "\n",
    "ex_per_class = 4\n",
    "EPOCHS = 10\n",
    "PATH = \"/Users/ninawiedemann/Desktop/UNI/Praktikum/sv_data.csv\"\n",
    "LABELS = \"Pitch Type\"\n",
    "act = leaky_relu\n",
    "CUT_OFF_Classes = 60\n",
    "\n",
    "prepro = Preprocessor(PATH, CUT_OFF_Classes)\n",
    "data = np.load(\"coord_sv.npy\") #prepro.get_coord_arr(\"coord_sv.npy\")\n",
    "\n",
    "M,N,nr_joints,_ = data.shape\n",
    "SEP = int(M*0.9)\n",
    "\n",
    "labels, unique = prepro.get_labels_onehot(LABELS)\n",
    "labels_string = prepro.get_labels(LABELS)\n",
    "#labels_test = decode_one_hot(labels[SEP:, :], unique)\n",
    "\n",
    "nr_classes = len(np.unique(labels_string))\n",
    "BATCHSIZE = nr_classes*ex_per_class\n",
    "print(\"nr classes\", nr_classes, \"Batchsize\", BATCHSIZE)\n",
    "\n",
    "# NET\n",
    "\n",
    "ind = np.random.permutation(len(data))\n",
    "train_ind = ind[:SEP]\n",
    "test_ind = ind[SEP:]\n",
    "\n",
    "train_x = data[train_ind]\n",
    "test_x = data[test_ind]\n",
    "train_t= labels[train_ind]\n",
    "test_t = labels[test_ind]\n",
    "labels_string_train = labels_string[train_ind]\n",
    "labels_string_test = labels_string[test_ind]\n",
    "\n",
    "\"\"\"\n",
    "DATA TESTING:\n",
    "indiuh = np.where(ind==2000)\n",
    "print(\"Labels nach preprocc von 2000\", labels_string[2000])\n",
    "print(\"new Index of 2000\", indiuh, \"test ob where funkt: \", ind[indiuh])\n",
    "print(\"train coord of 2000 u 140\", train_x[indiuh, 140])\n",
    "print(\"labels_string von 2000\", labels_string_train[indiuh])\n",
    "print(\"one hot von 2000\", train_t[indiuh])\n",
    "\"\"\"\n",
    "\n",
    "index_liste = []\n",
    "for pitches in unique:\n",
    "    index_liste.append(np.where(labels_string_train==pitches))\n",
    "\n",
    "len_test = len(test_x)\n",
    "len_train = len(train_x)\n",
    "print(\"Test set size: \", len_test, \" train set size: \", len_train)\n",
    "print(\"Shapes of train_x\", train_x.shape, \"shape of test_x\", test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normal conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, N, nr_joints, 2), name = \"input\")\n",
    "x_ = tf.reshape(x, (-1, N, nr_joints*2))\n",
    "y = tf.placeholder(tf.float32, (None, len(labels[0])))\n",
    "\n",
    "net = tf.layers.conv1d(x_, filters=256, kernel_size=5, strides=2, activation=act)\n",
    "net = tf.layers.conv1d(net, filters=256, kernel_size=3, strides=1, activation=act)\n",
    "net = tf.layers.conv1d(net, filters=128, kernel_size=3, strides=1, activation=act)\n",
    "net = tf.layers.conv1d(net, filters=1, kernel_size=1, activation = act)\n",
    "shapes = net.get_shape().as_list()\n",
    "ff = tf.reshape(net, (-1, shapes[1]*shapes[2]))\n",
    "ff = tf.layers.dense(ff, 1024, activation = act)\n",
    "ff = tf.layers.dense(ff, 128, activation = act)\n",
    "logits = tf.layers.dense(ff, len(labels[0]), activation = None)\n",
    "out = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)) # tf.reduce_mean(tf.square(y-ff))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = BATCHSIZE\n",
    "display_step = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network Parameters\n",
    "n_input = nr_joints*2 # MNIST data input (img shape: 28*28)\n",
    "n_steps = N # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 12 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x_ = tf.placeholder(\"float\", [None, n_steps, nr_joints, 2])\n",
    "x = tf.reshape(x_, (-1, N, nr_joints*2))\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\"\"\"# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def RNN(x):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    \"\"\"with tf.variable_scope(\"myrnn\") as scope:\n",
    "        for i in range(n_steps-1):\n",
    "            if i > 0:\n",
    "                scope.reuse_variables()\n",
    "            output, state = lstm_cell(x[i], state)\"\"\"\n",
    "\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.layers.dense(outputs[-1], n_classes)   #tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "out_logits = RNN(x)\n",
    "out = tf.nn.softmax(out_logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi layer lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = BATCHSIZE\n",
    "display_step = 10\n",
    "nr_layers = 4\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network Parameters\n",
    "n_input = nr_joints*2 # MNIST data input (img shape: 28*28)\n",
    "n_steps = N # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 12 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x_ = tf.placeholder(\"float\", [None, n_steps, nr_joints, 2])\n",
    "x = tf.reshape(x_, (-1, N, nr_joints*2))\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\"\"\"# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def RNN(x):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    #lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    def lstm_cell():\n",
    "          return rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(nr_layers)])\n",
    "\n",
    "\n",
    "    # Get lstm cell output\n",
    "    \"\"\"with tf.variable_scope(\"myrnn\") as scope:\n",
    "        for i in range(n_steps-1):\n",
    "            if i > 0:\n",
    "                scope.reuse_variables()\n",
    "            output, state = lstm_cell(x[i], state)\"\"\"\n",
    "\n",
    "    outputs, states = rnn.static_rnn(stacked_lstm, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.layers.dense(outputs[-1], n_classes)   #tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "out_logits = RNN(x)\n",
    "out = tf.nn.softmax(out_logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss test 2.09129\n",
      "Accuracy test:  0.0134831460674\n",
      "Accuracy train:  0.0189762796504\n",
      "Loss test 2.02599\n",
      "Accuracy test:  0.0\n",
      "Accuracy train:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0e2aed3b8778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbalanced_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCHSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# print(\"Loss test: \", sess.run(loss, {x: test_x, y: test_t}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#print(\"Loss train: \", sess.run(loss, {x: train_x, y: train_t}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ninawiedemann/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ninawiedemann/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ninawiedemann/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/ninawiedemann/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ninawiedemann/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batches(x, y, batchsize=32):\n",
    "    permute = np.random.permutation(len(x))\n",
    "    for i in range(0, len(x)-batchsize, batchsize):\n",
    "        indices = permute[i:i+batchsize]\n",
    "        yield x[indices], y[indices]\n",
    "\n",
    "def balanced_batches(x, y, batchsize=32):\n",
    "    for j in range(200):\n",
    "        liste=np.zeros((nr_classes, ex_per_class))\n",
    "        for i in range(nr_classes):\n",
    "            # print(j, i, np.random.choice(index_liste[i][0], ex_per_class))\n",
    "            liste[i] = np.random.choice(index_liste[i][0], ex_per_class, replace=False)\n",
    "        liste = liste.flatten().astype(int)\n",
    "        yield x[liste], y[liste]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run session for EPOCH epochs\n",
    "for epoch in range(EPOCHS + 1):\n",
    "    for batch_x, batch_t in balanced_batches(train_x, train_t, BATCHSIZE):\n",
    "        sess.run(optimizer, {x_: batch_x, y: batch_t})\n",
    "    # print(\"Loss test: \", sess.run(loss, {x: test_x, y: test_t}))\n",
    "    #print(\"Loss train: \", sess.run(loss, {x: train_x, y: train_t}))\n",
    "\n",
    "    #Test Accuracy\n",
    "    loss_test, out_test = sess.run([loss,out], {x_: test_x, y: test_t})\n",
    "    print(\"Loss test\", loss_test)\n",
    "    pitches_test = decode_one_hot(out_test, unique)\n",
    "    print(\"Accuracy test: \", np.sum(np.asarray(labels_string_test)==pitches_test)/len_test)\n",
    "    \n",
    "    #Train Accuracy\n",
    "    out_train = sess.run(out, {x_: train_x, y: train_t})\n",
    "    pitches_train = decode_one_hot(out_train, unique)\n",
    "    print(\"Accuracy train: \", np.sum(np.asarray(labels_string_train)==pitches_train)/SEP)\n",
    "\n",
    "print(pitches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ine': 0.6666666666666666, 'bdeu': 1.0, 'nina': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "out = np.array([\"ine\", \"ine\", \"bdeu\", \"hdba\", \"nina\", \"nina\"])\n",
    "ground_truth = np.array([\"ine\",\"ine\", \"bdeu\", \"ine\", \"nina\", \"halo\"])\n",
    "\n",
    "same = out[np.where(out==ground_truth)]\n",
    "\n",
    "right_frequency = sp.stats.itemfreq(same)\n",
    "total_frequency = sp.stats.itemfreq(ground_truth)\n",
    "right_dict = dict(zip(right_frequency[:,0], right_frequency[:,1]))\n",
    "total_dict = dict(zip(total_frequency[:,0], total_frequency[:,1]))\n",
    "\n",
    "acc = right_dict\n",
    "for types in right_dict.keys():\n",
    "    acc[types] = (int(right_dict[types])/float(total_dict[types]))\n",
    "    \n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
