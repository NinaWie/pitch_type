{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from os import listdir\n",
    "import time\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "\n",
    "Specify paths in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"/Users/ninawiedemann/Desktop/UNI/Praktikum/models/research/object_detection/\"\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/models/research/\")\n",
    "sys.path.append(path)\n",
    "\n",
    "# PATH_TO_TEST_IMAGES_DIR = 'test_images/video3'\n",
    "# TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, image_x) for image_x in listdir(PATH_TO_TEST_IMAGES_DIR)]\n",
    "#TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
    "# print(TEST_IMAGE_PATHS)\n",
    "\n",
    "batter_videos = \"/Volumes/Nina Backup/high_quality_testing/batter/\"\n",
    "name = \"#20 Dylan Cook (5).mp4\"\n",
    "\n",
    "INPUT_VIDEO_PATH = os.path.join(batter_videos, name)\n",
    "\n",
    "# IF THE OUTPUT IMAGES SHOULD BE SAVED: Set first variable true and specify output directory\n",
    "SHOW_IMAGES = True\n",
    "SAVE_IMAGES = False\n",
    "TEST_OUT_PATH = \"output_examples/video5/\"\n",
    "if not os.path.exists(TEST_OUT_PATH):\n",
    "    os.makedirs(TEST_OUT_PATH)\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = \"rfcn_resnet101_coco_11_06_2017\"\n",
    "# fourth_best: \"rfcn_resnet101_coco_11_06_2017\"\n",
    "# third_best: \"faster_rcnn_resnet101_coco_11_06_2017\"\n",
    "# best ging nicht: \"faster_rcnn_nas_coco_24_10_2017\"\n",
    "# second_best: \"faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\"\n",
    "#worst: 'ssd_mobilenet_v1_coco_11_06_2017'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = path+ MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join(path, 'data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model - does not need to be executed for most of the models because already loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "    file_name = os.path.basename(file.name)\n",
    "    if 'frozen_inference_graph.pb' in file_name:\n",
    "        tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haaalo <class 'object_detection.protos.string_int_label_map_pb2.StringIntLabelMap'> \n"
     ]
    }
   ],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run script on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080, 1920, 3)\n",
      "(1, 300, 4) (1, 300) (1, 300) (1,)\n",
      "Time for session:  30.724731922149658\n",
      "boxes [[ 168.53264451  729.15985107  364.35690522  821.01768494]\n",
      " [ 677.08420515  241.73415184  760.98715782  345.43604851]\n",
      " [ 155.99941134  698.86922836  414.27470326  820.17419815]\n",
      " [ 302.63798833  127.27981567  384.51003671  212.003088  ]\n",
      " [ 671.50036097  252.49986649  747.5973773   313.31726074]\n",
      " [ 266.57175064  711.13283157  286.12627745  723.89505386]\n",
      " [ 260.31583607  123.31749916  388.00043821  230.00121117]\n",
      " [ 249.33273733  710.87596893  274.9790597   726.1390686 ]\n",
      " [ 272.34169722   71.47946119  389.21683073  259.3415451 ]\n",
      " [ 166.54306769   73.20828438  721.85814857  939.95487213]\n",
      " [ 587.16501474  115.13846397  713.99549961  189.83267784]\n",
      " [ 676.58537865  179.66302872  850.61827898  372.53099442]\n",
      " [ 652.23383904  154.97573376  794.12497044  374.72963333]\n",
      " [ 198.52781475  700.68511963  225.29533803  726.10771179]\n",
      " [ 206.24718547   29.18799877  644.59836245  262.17418671]\n",
      " [ 545.07413864  223.93927574  812.03560352  363.48464012]\n",
      " [ 219.92672503   38.80643606  657.64368296  259.21545982]\n",
      " [ 193.59489441  709.06946182  211.20846212  731.36198044]\n",
      " [ 259.19128776   52.25234985  705.65835714  353.97233963]\n",
      " [ 672.29195595  277.33906746  712.46445179  328.60081673]\n",
      " [ 248.76242459   42.37243652  297.96434641  193.1621933 ]]\n",
      "scores [  8.88546050e-01   1.57738224e-01   8.90741721e-02   1.91681962e-02\n",
      "   5.29495114e-03   2.65799393e-03   2.38815672e-03   2.09721713e-03\n",
      "   1.82527932e-03   1.45119347e-03   1.34682131e-03   9.65481391e-04\n",
      "   8.88862123e-04   7.81499897e-04   7.61044270e-04   6.90654851e-04\n",
      "   6.53945492e-04   6.25003886e-04   5.94614423e-04   5.46109164e-04\n",
      "   5.32013888e-04]\n",
      "classes [ 39.  40.  39.  40.  40.  39.  40.  39.  40.  39.  40.  40.  40.  39.  39.\n",
      "  40.  40.  39.  39.  40.  39.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'IMAGE_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d6ce6c5e6bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mmax_boxes_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                   line_thickness=8)\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IMAGE_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "\n",
    "output_dic = {}\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        frame_count = 0\n",
    "        while frame_count<3:\n",
    "            ret, image_np = cap.read()\n",
    "            if image_np is None:\n",
    "                break\n",
    "            # image_np = frame \n",
    "            frame_count+=1\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            print(image_np_expanded.shape)\n",
    "            # Actual detection.\n",
    "            tic = time.time()\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "              [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "            print(boxes.shape, scores.shape, classes.shape, num.shape)\n",
    "            # Visualization of the results of a detection.\n",
    "            print(\"Time for session: \", time.time()-tic)\n",
    "            \n",
    "            index = np.union1d(np.where(np.squeeze(classes)==40)[0],np.where(np.squeeze(classes)==39)[0]) #np.where(np.squeeze(classes)!=1)[0]\n",
    "            # print(index)\n",
    "            classes = np.squeeze(classes)[index]\n",
    "            scores = np.squeeze(scores)[index]\n",
    "            boxes = np.squeeze(boxes)[index]\n",
    "            num = len(classes)\n",
    "            print(\"boxes\", boxes*np.array([[1080,1920,1080, 1920] for _ in range(len(boxes))]))\n",
    "            print(\"scores\", scores)\n",
    "            print(\"classes\", classes)\n",
    "            \n",
    "            inner_dic = {}\n",
    "            for i, s in enumerate(scores):\n",
    "                if s>0.6:\n",
    "                    if classes[i]==39:\n",
    "                        class_name = \"bat\"\n",
    "                    else:\n",
    "                        class_name = \"glove\"\n",
    "                    inner_dic[name] = {\"score\":float(s), \"box\":boxes[i].tolist()}\n",
    "            output_dic[str(frame_count).zfill(4)] = inner_dic\n",
    "            if SHOW_IMAGES:\n",
    "                vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np,\n",
    "                  boxes, classes.astype(np.int32), scores,\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                    min_score_thresh = 0.6,\n",
    "                    max_boxes_to_draw = 100,\n",
    "                  line_thickness=8)\n",
    "                plt.figure(figsize=IMAGE_SIZE)\n",
    "                plt.imshow(image_np)\n",
    "                plt.show()\n",
    "                if SAVE_IMAGES:\n",
    "                    cv2.imwrite(TEST_OUT_PATH+image_path.split(\"/\")[-1], image_np)\n",
    "                    #plt.savefig()\n",
    "print(output_dic)\n",
    "with open(\"outputs/\"+name+ \".json\", \"w\") as outfile:\n",
    "    json.dump(output_dic, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.where(classes==40)[0])\n",
    "print(np.where(classes==39)[0])\n",
    "print(classes)\n",
    "print(np.union1d(np.where(classes==40)[0],np.where(classes==39)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39.]\n"
     ]
    }
   ],
   "source": [
    "print(classes[np.where(np.union1d(classes== 47, classes== 39))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n",
      "(49, 4) (49,) (49,) 49\n"
     ]
    }
   ],
   "source": [
    "index = np.where(np.squeeze(classes)!=1)[0]\n",
    "print(index)\n",
    "classes = np.squeeze(classes)[index]\n",
    "scores = np.squeeze(scores)[index]\n",
    "boxes = np.squeeze(boxes)[index]\n",
    "num = len(classes)\n",
    "print(boxes.shape, scores.shape, classes.shape, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"#20 Dylan Cook (5).mp4\" #\"#22 Jordan Winawer (3).mp4\"\n",
    "path = \"/Volumes/Nina Backup/high_quality_testing/batter/\"\n",
    "# \"/Users/ninawiedemann/Desktop/UNI/Praktikum/high_quality_testing/batter/\"\n",
    "cap = cv2.VideoCapture(path+name)\n",
    "p=0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    cv2.imwrite(\"/Users/ninawiedemann/Desktop/UNI/Praktikum/models/research/object_detection/test_images/video3/pic\"+str(p)+\".jpg\", frame)\n",
    "    p+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        for image_path in TEST_IMAGE_PATHS[120:170]:\n",
    "            if image_path[-4:] != \".jpg\":\n",
    "                continue\n",
    "            print(image_path, TEST_OUT_PATH+image_path.split(\"/\")[-1])\n",
    "            image = Image.open(image_path)\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            \n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            print(image_np_expanded.shape)\n",
    "            # Actual detection.\n",
    "            tic = time.time()\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "              [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "            print(boxes.shape, scores.shape, classes.shape, num.shape)\n",
    "            # Visualization of the results of a detection.\n",
    "            print(\"Time for session: \", time.time()-tic)\n",
    "            \n",
    "            index = np.union1d(np.where(np.squeeze(classes)==40)[0],np.where(np.squeeze(classes)==39)[0]) #np.where(np.squeeze(classes)!=1)[0]\n",
    "            print(index)\n",
    "            classes = np.squeeze(classes)[index]\n",
    "            scores = np.squeeze(scores)[index]\n",
    "            boxes = np.squeeze(boxes)[index]\n",
    "            num = len(classes)\n",
    "            \n",
    "            print(scores)\n",
    "            print(classes)\n",
    "\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "              image_np,\n",
    "              boxes, classes.astype(np.int32), scores,\n",
    "              category_index,\n",
    "              use_normalized_coordinates=True,\n",
    "                min_score_thresh = 0.001,\n",
    "                max_boxes_to_draw = 100,\n",
    "              line_thickness=8)\n",
    "            plt.figure(figsize=IMAGE_SIZE)\n",
    "            plt.imshow(image_np)\n",
    "            plt.show()\n",
    "            cv2.imwrite(TEST_OUT_PATH+image_path.split(\"/\")[-1], image_np)\n",
    "            #plt.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old faster rcnn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASSES = ('background',\n",
    "'person', 'bicycle', 'car','motorcycle','airplane','bus','train', 'truck', 'boat','traffic light',\n",
    "'fire hydrant', 'stop sign', 'parking meter', 'bench','bird','cat','dog','horse','sheep',\n",
    "'cow', 'elephant','bear','zebra','giraffe','hat','umbrella', 'handbag','tie','suitcase',\n",
    "'frisbee','skis','snowboard','sports ball','kite', 'baseball bat','baseball glove','skateboard','surfboard','tennis racket',\n",
    "'bottle','wine glass','cup','fork','knife','spoon','bowl','banana','apple','sandwich',\n",
    "'orange','broccoli','carrot','hot dog','pizza','donut','cake','chair','couch','potted plant',\n",
    "'bed','dining table','window','tv','laptop','mouse','remote','keyboard','cell phone','microwave',\n",
    "'oven', 'sink','refrigerator','blender','book','clock','vase','scissors','teddy bear','hair drier','tooth brush')\n",
    "\n",
    "def vis_detections(im, class_name, dets, thresh):\n",
    "    \n",
    "    \"\"\"Draw detected bounding boxes.\"\"\"\n",
    "    inds = np.where(dets[:, -1] >= thresh)[0]\n",
    "    #if len(inds) == 0:\n",
    "    #    return\n",
    "\n",
    "    im = im[:, :, (2, 1, 0)]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    ax.imshow(im, aspect='equal')\n",
    "    \n",
    "    for i in inds:\n",
    "        bbox = dets[i, :4]\n",
    "        score = dets[i, -1]\n",
    "\n",
    "        ax.add_patch(\n",
    "            plt.Rectangle((bbox[0], bbox[1]),\n",
    "                          bbox[2] - bbox[0],\n",
    "                          bbox[3] - bbox[1], fill=False,\n",
    "                          edgecolor='red', linewidth=3.5)\n",
    "            )\n",
    "    \n",
    "        ax.text(bbox[0], bbox[1] - 2,\n",
    "                '{:s} {:.3f}'.format(class_name, score),\n",
    "                bbox=dict(facecolor='blue', alpha=0.5),\n",
    "                fontsize=14, color='white')\n",
    "\n",
    "    ax.set_title(('{} detections with '\n",
    "                  'p({} | box) >= {:.1f}').format(class_name, class_name,\n",
    "                                                  thresh),\n",
    "                  fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "    \n",
    "    return bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        \n",
    "centerBat = []\n",
    "batBox = []\n",
    "isBat=[0]*9\n",
    "counter=0\n",
    "from os import listdir\n",
    "path = path_all + \"outputpic/\"\n",
    "for f in sorted(listdir(path)):\n",
    "    if ((f[0]!=\".\") and (int(f[7]) ==6)):\n",
    "        # print(f[:4])\n",
    "        im = plt.imread(path_all+\"inputpic/\"+f[:4]+\".jpg\")\n",
    "        if f == \"0102_0.6\":\n",
    "            fra = im\n",
    "        with open(path+f, \"r\") as infile:\n",
    "            try:\n",
    "                dets_dic = json.load(infile)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        CONF_THRESH = float(f[5:])\n",
    "        NMS_THRESH = 0.2\n",
    "\n",
    "        for cls_ind, cls in enumerate(CLASSES[35:36]):\n",
    "            cls_ind += 1 # because we skipped background\n",
    "            try:\n",
    "#                 if int(f[7])==6:\n",
    "                dets = dets_dic[cls]\n",
    "                x1, y1, x2, y2 = list(vis_detections(im, cls, np.array(dets), thresh=CONF_THRESH))\n",
    "                batBox.append([np.array([[x1,y1],[x2,y2]]), int(f[:4])-1])\n",
    "#                 centerBat.append([(x2+x1)/2, (y2+y1)/2, int(f[:4])-1])\n",
    "\n",
    "                isBat[int(f[7])-1]+=1\n",
    "                counter+=1\n",
    "                # print(\"detection\", counter)\n",
    "            except KeyError:\n",
    "                # print(\"no detection\", counter)\n",
    "                continue\n",
    "                \n",
    "batBox = np.array(batBox)\n",
    "batLenCap = max(np.linalg.norm(batBox[i,0][0]-batBox[i,0][1]) for i in range(len(batBox)))\n",
    "oldBatBox = batBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batTip = []\n",
    "# batBase = []\n",
    "# # wristBase = []\n",
    "# for i in range(len(batBox)):\n",
    "#     frame = batBox[i][1]\n",
    "# #     wristBase.append([wristTracker[frame][0], wristTracker[frame][1],frame])\n",
    "#     if abs(batBox[i][0][0]-wristTracker[frame][0]) > abs(batBox[i][0][2]-wristTracker[frame][0]):\n",
    "#         xTipInd = 0\n",
    "#     else:\n",
    "#         xTipInd = 2\n",
    "#     if abs(batBox[i][0][1]-wristTracker[frame][1]) > abs(batBox[i][0][3]-wristTracker[frame][1]):\n",
    "#         yTipInd = 1\n",
    "#     else:\n",
    "#         yTipInd = 3\n",
    "#     batTip.append([batBox[i][0][xTipInd],batBox[i][0][yTipInd], frame])\n",
    "#     batBase.append([batBox[i][0][abs(xTipInd-2)],batBox[i][0][abs(yTipInd-4)], frame])\n",
    "    \n",
    "# tip = []\n",
    "# base = []\n",
    "# for i in range(len(batTip)):\n",
    "#      if batTip[i][2] in range(80,150):\n",
    "#         tip.append(batTip[i])\n",
    "#         base.append(batBase[i])\n",
    "\n",
    "# tip = np.array(tip)\n",
    "# base = np.array(base)\n",
    "# batLenCap = max(np.linalg.norm(tip[i][:2]-base[i][:2]) for i in range(len(tip)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMO C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE = \"/Volumes/Nina Backup/ColeLoncar/\" # os.path.join('BatDetectHQ', 'ColeLoncar')\n",
    "VIDEO = os.path.join(BASE, 'inputvid/test.mp4')\n",
    "# IMG = os.path.join(BASE,'inputpic/')\n",
    "IMGS = \"/Volumes/Nina Backup/ColeLoncar/inputpic/\"\n",
    "length = 300\n",
    "# We read the images as grayscale so they are ready for thresholding functions.\n",
    "images = []\n",
    "images = [cv2.imread(IMGS+\"%04d.jpg\"%idx, cv2.IMREAD_GRAYSCALE) for idx in range(1,length+1)] #IMG_TEMPLATE.format(idx), ) \n",
    "#fig = plt.figure(figsize=(18, 16), edgecolor='k')\n",
    "#plt.imshow(images[162])\n",
    "#plt.show()\n",
    "\n",
    "# blurBatTip = []\n",
    "# blurBatBase = []\n",
    "\n",
    "blurBat = []\n",
    "\n",
    "candidate_values = []\n",
    "whiteness_values = []\n",
    "location = []\n",
    "frame_indizes = []\n",
    "t=0\n",
    "frame_before_close_wrist = False\n",
    "\n",
    "# currentTip = tip[0,:2]\n",
    "# currentBase = base[0,:2]\n",
    "# currentInd = tip[0,2]\n",
    "currentInd = batBox[0,1]\n",
    "inders = []\n",
    "\n",
    "for t in missingInd:#range(1, length-2):\n",
    "    for ind in range(int(currentInd), t):\n",
    "        if ind in batBox[:,1]:\n",
    "            loca = np.where(batBox[:,1]==ind)\n",
    "            loca = loca[0][0]\n",
    "            currentBat = batBox[loca,0]\n",
    "\n",
    "#         if ind in tip[:,2]:\n",
    "#             loca = np.where(tip[:,2]==ind)\n",
    "#             loca = loca[0][0]\n",
    "#             currentTip = tip[loca,:2]\n",
    "#             currentBase = base[loca,:2]\n",
    "#             currentInd = ind\n",
    "\n",
    "    im_tm1 = images[t - 1]\n",
    "    im_t = images[t]\n",
    "    im_tp1 = images[t + 1]\n",
    "    \n",
    "#     print images[t-1].shape\n",
    "#     print images[t].shape\n",
    "#     print images[t+1].shape\n",
    "    print(t)\n",
    "    delta_plus = cv2.absdiff(im_t, im_tm1)\n",
    "    delta_0 = cv2.absdiff(im_tp1, im_tm1)\n",
    "    delta_minus = cv2.absdiff(im_t,im_tp1)\n",
    "    sp = cv2.meanStdDev(delta_plus)\n",
    "    sm = cv2.meanStdDev(delta_minus)\n",
    "    s0 = cv2.meanStdDev(delta_0)\n",
    "    #print(\"E(d+):\", sp, \"\\nE(d-):\", sm, \"\\nE(d0):\", s0)\n",
    "\n",
    "\n",
    "    th = [\n",
    "        sp[0][0, 0] + 3 * math.sqrt(sp[1][0, 0]),\n",
    "        sm[0][0, 0] + 3 * math.sqrt(sm[1][0, 0]),\n",
    "        s0[0][0, 0] + 3 * math.sqrt(s0[1][0, 0]),\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # OPENCV THRESHOLD\n",
    "    start = time.time()\n",
    "    ret, dbp = cv2.threshold(delta_plus, th[0], 255, cv2.THRESH_BINARY)\n",
    "    ret, dbm = cv2.threshold(delta_minus, th[1], 255, cv2.THRESH_BINARY)\n",
    "    ret, db0 = cv2.threshold(delta_0, th[2], 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    detect = cv2.bitwise_not(cv2.bitwise_and(cv2.bitwise_and(dbp, dbm), \n",
    "                    cv2.bitwise_not(db0)))\n",
    "\n",
    "    ocv_time = (time.time() - start) * 1000\n",
    "    \n",
    "    \n",
    "    # CONNECTED BOX\n",
    "    # The original `detect` image was suitable for display, but it is \"inverted\" and not suitable\n",
    "    # for component detection; we need to invert it first.\n",
    "    start = time.time()\n",
    "    nd = cv2.bitwise_not(detect)\n",
    "    num, labels, stats, centroids = cv2.connectedComponentsWithStats(nd, ltype=cv2.CV_16U)\n",
    "\n",
    "\n",
    "    # We set an arbitrary threshold to screen out smaller \"components\"\n",
    "    # which may result simply from noise, or moving leaves, and other\n",
    "    # elements not of interest.\n",
    "    min_area = 500\n",
    "\n",
    "\n",
    "    d = detect.copy()\n",
    "    candidates = list()\n",
    "    for stat in stats[1:]:\n",
    "        area = stat[cv2.CC_STAT_AREA]\n",
    "        if area < min_area:\n",
    "            continue # Skip small objects (noise)\n",
    "\n",
    "        lt = (stat[cv2.CC_STAT_LEFT], stat[cv2.CC_STAT_TOP])\n",
    "        rb = (lt[0] + stat[cv2.CC_STAT_WIDTH], lt[1] + stat[cv2.CC_STAT_HEIGHT])                                                \n",
    "        bottomLeftCornerOfText = (lt[0], lt[1] - 15)\n",
    "\n",
    "        candidates.append((lt, rb, area))\n",
    "\n",
    "    \n",
    "    #plt.figure(figsize=(10, 10), edgecolor='k')\n",
    "    \n",
    "    # MOTION MODEL\n",
    "    \n",
    "    # Thinning threshold.\n",
    "    psi = 0.7\n",
    "\n",
    "    # Area matching threshold.\n",
    "#     gamma = 0.4\n",
    "    gamma = 20\n",
    "    area_max = 0\n",
    "#     area_min = 1\n",
    "\n",
    "    fom_detected = False\n",
    "    start = time.time()\n",
    "    col = len(candidates)\n",
    "    sub_regions = list()\n",
    "    \n",
    "    \n",
    "    index = []\n",
    "    close_to_wrist = False\n",
    "\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        # The first two elements of each `candidate` tuple are\n",
    "        # the opposing corners of the bounding box.\n",
    "        x1, y1 = candidate[0]\n",
    "        x2, y2 = candidate[1]\n",
    "        \n",
    "        center = [(x1+x2)/2,(y1+y2)/2]\n",
    "\n",
    "        actual_area = candidate[2]\n",
    "        cand = nd[y1:y2, x1:x2]\n",
    "        dt = cv2.distanceTransform(cand, distanceType=cv2.DIST_L2, maskSize=cv2.DIST_MASK_PRECISE)\n",
    "        radius = np.amax(dt)\n",
    "        ret, Pt = cv2.threshold(dt, psi * radius, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        path_len = math.sqrt(w * w + h * h)\n",
    "        expected_area = radius * (2 * path_len + math.pi * radius)\n",
    "\n",
    "        area_ratio = abs(actual_area / expected_area - 1)\n",
    "\n",
    "        location.append(center)\n",
    "        index.append(i)\n",
    "        area = candidates[i][2]\n",
    "        candidate_values.append(area_ratio)\n",
    "        patch = im_t[y1:y2, x1:x2]\n",
    "        whiteness_values.append(np.mean(patch))\n",
    "        frame_indizes.append(t-1)   \n",
    "    \n",
    "    \n",
    "    if len(candidates)>0:\n",
    "        print(\"DETECTED\", t)#, whiteness_values[-1], candidate_values[-1])\n",
    "        inders.append(t)\n",
    "        plt.figure(figsize=(10, 10), edgecolor='r')\n",
    "        candidates = combineOverlapping(candidates)\n",
    "#         print candidates\n",
    "        img = np.tile(np.expand_dims(im_t.copy(), axis = 2), (1,1,3))\n",
    "        for fom in index:\n",
    "            cv2.rectangle(img, candidates[fom][0], candidates[fom][1],[255,0,0], 4)\n",
    "#         wristTup = (int(wristTracker[t][0])-10,int(wristTracker[t][1])-10)\n",
    "#         wristTupA = (int(wristTracker[t][0])+10,int(wristTracker[t][1])+10)\n",
    "#         cv2.rectangle(img, wristTup , wristTupA, [0,255,0], 4)\n",
    "\n",
    "        plt.imshow(img, 'gray')\n",
    "        plt.title(\"Detected FOM\".format(t))\n",
    "        plt.show()\n",
    "        \n",
    "        newBox = closestBox(candidates, currentBat)\n",
    "        if newBox.any():\n",
    "            blurBat.append([np.array(newBox), int(t)])\n",
    "        \n",
    "#         theTip, theBase = findBatVerts(candidates,currentTip)\n",
    "            \n",
    "#         if (theTip, theBase) != (-1,-1):\n",
    "#             blurBatTip.append([theTip[0],theTip[1], t])\n",
    "#             blurBatBase.append([theBase[0],theBase[1], t])\n",
    "#             currentTip = theTip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def findBatVerts(cands,tipVert):\n",
    "#     minVal = 2*batLenCap\n",
    "#     minInd = (-1, -1)\n",
    "#     for i in range(len(cands)):\n",
    "#         rec = np.array(cands[i][:2])\n",
    "#         for xInd in range(2):\n",
    "#             xVal = rec[xInd,0]\n",
    "#             for yInd in range(2):\n",
    "#                 yVal = rec[yInd,1]\n",
    "#                 if np.linalg.norm(tipVert - [xVal,yVal]) < minVal:\n",
    "#                     minVal = np.linalg.norm(tipVert - [xVal,yVal])\n",
    "#                     minInd = xInd, yInd\n",
    "#                     candLoc = i\n",
    "#     if minInd != (-1,-1):\n",
    "#         rec = np.array(cands[candLoc][:2])\n",
    "#         newTip = rec[minInd[0], 0], rec[minInd[1], 1]\n",
    "#         newBase = rec[abs(minInd[0]-1), 0], rec[abs(minInd[1]-1), 1]\n",
    "#         return newTip, newBase\n",
    "#     else:\n",
    "#         return -1, -1\n",
    "      \n",
    "    \n",
    "# blurTip = np.array(blurBatTip)\n",
    "# blurBase = np.array(blurBatBase)\n",
    "# combTip = np.concatenate((tip,blurTip),axis=0)\n",
    "# combTip =  combTip[combTip[:,2].argsort()]\n",
    "# combBase = np.concatenate((base,blurBase),axis=0)\n",
    "# combBase =  combBase[combBase[:,2].argsort()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 2D plotting of base and tip\n",
    "plt.plot(base[:,0], base[:,1])\n",
    "plt.ylim(650,300)\n",
    "\n",
    "for i in range(len(base)):\n",
    "    plt.annotate(base[i][2], (base[:,0][i], base[:,1][i]))\n",
    "\n",
    "plt.xlabel()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display wrist, base and tip images in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = -1\n",
    "\n",
    "for t in tip[:,2]:\n",
    "    i += 1\n",
    "    print(\"DETECTED\", t)\n",
    "    im_t =images[int(t)]\n",
    "#     loca = np.where(tip[:,2]==int(t))\n",
    "#     if (len(loca[0]) == 0):\n",
    "#         color = [0,0,255]\n",
    "#         loca = np.where(blurTip[:,2]==int(t))\n",
    "#         loca = loca[0][0]\n",
    "#         tipTup = (int(blurTip[loca][0])-10,int(blurTip[loca][1])-10)\n",
    "#         tipTupA = (int(blurTip[loca][0])+10,int(blurTip[loca][1])+10)\n",
    "#         baseTup = (int(blurBase[loca][0])-10,int(blurBase[loca][1])-10)\n",
    "#         baseTupA = (int(blurBase[loca][0])+10,int(blurBase[loca][1])+10)\n",
    "        \n",
    "#     else:\n",
    "#         color = [255,0,0]\n",
    "#         loca = loca[0][0]\n",
    "#         tipTup = (int(tip[loca][0])-10,int(tip[loca][1])-10)\n",
    "#         tipTupA = (int(tip[loca][0])+10,int(tip[loca][1])+10)\n",
    "#         baseTup = (int(base[loca][0])-10,int(base[loca][1])-10)\n",
    "#         baseTupA = (int(base[loca][0])+10,int(base[loca][1])+10)\n",
    "\n",
    "    color  = [0,0,255]\n",
    "    \n",
    "    plt.figure(figsize=(10, 10), edgecolor='r')\n",
    "    img = np.tile(np.expand_dims(im_t.copy(), axis = 2), (1,1,3))\n",
    "    tipTup = (int(tip[i][0])-10,int(tip[i][1])-10)\n",
    "    tipTupA = (int(tip[i][0])+10,int(tip[i][1])+10)\n",
    "    wristTup = (int(wristTracker[int(t)][0])-10,int(wristTracker[int(t)][1])-10)\n",
    "    wristTupA = (int(wristTracker[int(t)][0])+10,int(wristTracker[int(t)][1])+10)\n",
    "    baseTup = (int(base[i][0])-10,int(base[i][1])-10)\n",
    "    baseTupA = (int(base[i][0])+10,int(base[i][1])+10)\n",
    "\n",
    "    cv2.rectangle(img, tipTup , tipTupA, color, 4)\n",
    "    cv2.rectangle(img, wristTup , wristTupA, [0,255,0], 4)\n",
    "    cv2.rectangle(img, baseTup , baseTupA, color, 4)\n",
    "    \n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.title(\"Detected FOM\".format(int(t)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get only the part during the swing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swingBox = []\n",
    "for i in range(len(oldBatBox)):\n",
    "    if oldBatBox[i][1] in range(80,150)\n",
    "        swingBox.append(oldBatBox[i])       \n",
    "\n",
    "batBox = np.array(swingBox)\n",
    "missingInd = np.arange(80,150).tolist()\n",
    "\n",
    "for ind in batBox[:,1]:\n",
    "    if ind in missingInd:\n",
    "        missingInd.remove(ind)\n",
    "        \n",
    "    \n",
    "# tip = []\n",
    "# base = []\n",
    "# for i in range(len(batTip)):\n",
    "#      if batTip[i][2] in range(80,150):\n",
    "#         tip.append(batTip[i])\n",
    "#         base.append(batBase[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meine alte version von only box bilden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Threshold: two times the maximum length of the bat\n",
    "\n",
    "frame_count = 0\n",
    "currentBat = batBox[0,0]\n",
    "\n",
    "onlyBat = batBox.tolist()\n",
    "\n",
    "cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "\n",
    "# candidates_per_frame = candidates_per_frame[2:]\n",
    "# for i, cand_list in enumerate(candidates_per_frame):\n",
    "    \n",
    "while frame_count<len(candidates_per_frame):\n",
    "    ret, image_np = cap.read()\n",
    "    if image_np is None:\n",
    "        print(frame_count)\n",
    "        break\n",
    "    # print(frame_count, len(candidates_per_frame))\n",
    "    cand_list = candidates_per_frame[frame_count]\n",
    "    candidates = [[cand.bbox[:2], cand.bbox[2:]] for cand in cand_list]\n",
    "    if len(candidates)>1:\n",
    "        candidates = combineOverlapping(candidates)\n",
    "        # print(candidates)\n",
    "    \n",
    "    if len(candidates)>0:\n",
    "        # print(\"candidates\", candidates, \"currentBat\", currentBat)\n",
    "        newBox = closestBox(candidates, currentBat, MAX_DIST_THRESH)\n",
    "        # print(newBox)\n",
    "        if newBox.any():\n",
    "            onlyBat.append([np.array(newBox), int(frame_count)])\n",
    "            currentBat = newBox\n",
    "        \n",
    "        if PLOTTING:\n",
    "            img = image_np.copy()\n",
    "            for fom in range(len(candidates)):\n",
    "                cv2.rectangle(img, tuple(candidates[fom][0]), tuple(candidates[fom][1]),[255,0,0], 4)\n",
    "            plt.figure(figsize = (20,10))\n",
    "            plt.imshow(img, 'gray')\n",
    "            plt.title(\"Candidates in frame \"+str(frame_count))\n",
    "            plt.show()\n",
    "    frame_count+=1\n",
    "    \n",
    "# print(\"-----------------\")\n",
    "print(\"output: [first point of rectange, second point of rectangle, frame]\" )\n",
    "print(onlyBat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
